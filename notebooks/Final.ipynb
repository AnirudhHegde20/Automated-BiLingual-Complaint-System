{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final end to end working xgboost model with deployment"
      ],
      "metadata": {
        "id": "c-MkHSDT1Rrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kfp model_registry"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FZ6zGOSDbrXY",
        "outputId": "bdb6510b-a57a-4bba-d271-0c357356487d",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kfp\n",
            "  Downloading kfp-2.10.1.tar.gz (343 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/343.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m307.2/343.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting model_registry\n",
            "  Downloading model_registry-0.2.10-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: click<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (8.1.7)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.16)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.8.0)\n",
            "Collecting kfp-pipeline-spec==0.5.0 (from kfp)\n",
            "  Downloading kfp_pipeline_spec-0.5.0-py3-none-any.whl.metadata (293 bytes)\n",
            "Collecting kfp-server-api<2.4.0,>=2.1.0 (from kfp)\n",
            "  Downloading kfp_server_api-2.3.0.tar.gz (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kubernetes<31,>=8.0.0 (from kfp)\n",
            "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: protobuf<5,>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (4.25.5)\n",
            "Requirement already satisfied: PyYAML<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (6.0.2)\n",
            "Collecting requests-toolbelt<1,>=0.8.0 (from kfp)\n",
            "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.9.0)\n",
            "Collecting urllib3<2.0.0 (from kfp)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.9.5 in /usr/local/lib/python3.10/dist-packages (from model_registry) (3.10.10)\n",
            "Collecting aiohttp-retry<3.0.0,>=2.8.3 (from model_registry)\n",
            "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from model_registry) (0.2.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from model_registry) (1.6.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from model_registry) (2.9.2)\n",
            "Collecting python-dateutil<3.0.0,>=2.9.0.post0 (from model_registry)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.8 in /usr/local/lib/python3.10/dist-packages (from model_registry) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.5->model_registry) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.5->model_registry) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.5->model_registry) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.5->model_registry) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.5->model_registry) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.5->model_registry) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.5->model_registry) (4.0.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.25.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.7.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp) (2024.8.30)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<31,>=8.0.0->kfp) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<31,>=8.0.0->kfp) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes<31,>=8.0.0->kfp) (3.2.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->model_registry) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->model_registry) (2.23.4)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3,>=2.2.1->kfp) (1.6.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.9.5->model_registry) (0.2.0)\n",
            "Downloading kfp_pipeline_spec-0.5.0-py3-none-any.whl (9.1 kB)\n",
            "Downloading model_registry-0.2.10-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: kfp, kfp-server-api\n",
            "  Building wheel for kfp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp: filename=kfp-2.10.1-py3-none-any.whl size=364801 sha256=f8d78f5ec8a0fde07a07609fe0eacbe00ad5a79b1fdf5906112e5ca2b979ee11\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/3a/e6/68b6a5de8fec76d3e68c2ca3c7149c81aff126da47c105417d\n",
            "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp-server-api: filename=kfp_server_api-2.3.0-py3-none-any.whl size=116410 sha256=9b4da68dc751e786f75d659caaf619d104ca9225b33fb7679945863a2b8f9a1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/f2/84/e4da136339a1cf32250c1e1ba680a8d1e1c5ca07e322facb7e\n",
            "Successfully built kfp kfp-server-api\n",
            "Installing collected packages: urllib3, python-dateutil, kfp-pipeline-spec, kfp-server-api, requests-toolbelt, kubernetes, aiohttp-retry, model_registry, kfp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.2.3\n",
            "    Uninstalling urllib3-2.2.3:\n",
            "      Successfully uninstalled urllib3-2.2.3\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: requests-toolbelt\n",
            "    Found existing installation: requests-toolbelt 1.0.0\n",
            "    Uninstalling requests-toolbelt-1.0.0:\n",
            "      Successfully uninstalled requests-toolbelt-1.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.1.142 requires requests-toolbelt<2.0.0,>=1.0.0, but you have requests-toolbelt 0.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-retry-2.9.1 kfp-2.10.1 kfp-pipeline-spec-0.5.0 kfp-server-api-2.3.0 kubernetes-30.1.0 model_registry-0.2.10 python-dateutil-2.9.0.post0 requests-toolbelt-0.10.1 urllib3-1.26.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              },
              "id": "14bfa29e1b1c407e8be2af0de44d0873"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate your Google Cloud account\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import kfp\n",
        "import json\n",
        "from datetime import datetime\n",
        "from kfp import compiler, dsl\n",
        "from typing import NamedTuple, List, Union, Dict, Any\n",
        "from kfp.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics, Dataset\n",
        "from google.cloud import aiplatform\n",
        "from model_registry import ModelRegistry\n",
        "\n",
        "print(f'KFB version: {kfp.__version__}')\n",
        "\n",
        "\n",
        "\n",
        "PROJECT_ID = 'bilingualcomplaint-system'\n",
        "LOCATION = 'us-east1'\n",
        "# Bucket Name\n",
        "GCS_artifacts_bucket_name = 'tfx-artifacts'\n",
        "# Pipeline\n",
        "pipeline_name = 'complaints-clf-vertex-training'\n",
        "# Path to various pipeline artifact.\n",
        "_pipeline_artifacts_dir = f'gs://{GCS_artifacts_bucket_name}/pipeline_artifacts/{pipeline_name}'\n",
        "\n",
        "aiplatform.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        "    staging_bucket=f'gs://{GCS_artifacts_bucket_name}',\n",
        "    )\n",
        "\n",
        "@component(\n",
        "    base_image=\"python:3.10-slim\",\n",
        "    packages_to_install = [\n",
        "        'google-cloud-bigquery==3.26.0',\n",
        "        'pandas==1.5.3',\n",
        "        'numpy==1.26.4',\n",
        "        'db-dtypes==1.3.0',\n",
        "        'scikit-learn==1.5.2'\n",
        "        ]\n",
        "    )\n",
        "def get_data_component(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    start_year: int, end_year: int,\n",
        "    feature_name: str,\n",
        "    label_name: str,\n",
        "    train_data: Output[Dataset],\n",
        "    val_data: Output[Dataset],\n",
        "    testset_size: float = 0.2,\n",
        "    limit:int=200):\n",
        "\n",
        "  from google.cloud import bigquery\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  import smtplib\n",
        "  from email.mime.multipart import MIMEMultipart\n",
        "  from email.mime.text import MIMEText\n",
        "  import requests\n",
        "  from datetime import datetime\n",
        "\n",
        "  # Track the start time of the component execution\n",
        "  start_time = datetime.now()\n",
        "\n",
        "  # Function to send custom Slack message with Kubeflow component details\n",
        "  def send_slack_message(component_name, execution_date, execution_time, duration):\n",
        "      SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'  # Replace with your Slack webhook URL\n",
        "      message = {\n",
        "          \"attachments\": [\n",
        "              {\n",
        "                  \"color\": \"#36a64f\",  # Green color for success\n",
        "                  \"pretext\": \":large_green_circle: Kubeflow Component Success Alert\",\n",
        "                  \"fields\": [\n",
        "                      {\n",
        "                          \"title\": \"Component Name\",\n",
        "                          \"value\": component_name,\n",
        "                          \"short\": True\n",
        "                      },\n",
        "                      {\n",
        "                          \"title\": \"Execution Date\",\n",
        "                          \"value\": execution_date,\n",
        "                          \"short\": True\n",
        "                      },\n",
        "                      {\n",
        "                          \"title\": \"Execution Time\",\n",
        "                          \"value\": execution_time,\n",
        "                          \"short\": True\n",
        "                      },\n",
        "                      {\n",
        "                          \"title\": \"Duration\",\n",
        "                          \"value\": f\"{duration} minutes\",\n",
        "                          \"short\": True\n",
        "                      }\n",
        "                  ]\n",
        "              }\n",
        "          ]\n",
        "      }\n",
        "\n",
        "      try:\n",
        "          response = requests.post(SLACK_WEBHOOK_URL, json=message)\n",
        "          response.raise_for_status()  # Check for request errors\n",
        "          pass\n",
        "      except requests.exceptions.RequestException as e:\n",
        "          pass\n",
        "\n",
        "  # Function to send success email\n",
        "  def send_success_email():\n",
        "      sender_email = \"sucessemailtrigger@gmail.com\"\n",
        "      password = \"jomnpxbfunwjgitb\"\n",
        "      receiver_emails = [\"hegde.anir@northeastern.edu\",\n",
        "                         \"nenavath.r@northeastern.edu\",\n",
        "                         \"pandey.raj@northeastern.edu\",\n",
        "                         \"khatri.say@northeastern.edu\",\n",
        "                         \"singh.arc@northeastern.edu\",\n",
        "                         \"goparaju.v@northeastern.edu\"]\n",
        "\n",
        "      # Current time for logging purposes\n",
        "      current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "      # Create the email content\n",
        "      subject = '[Kubeflow Pipeline] - Started'\n",
        "      body = f'''Hi team,\n",
        "\n",
        "      Model training in the Kubeflow pipeline has started!\n",
        "\n",
        "      Details:\n",
        "      - Start Time: {current_time}\n",
        "      - Dataset: {start_year}-{end_year}\n",
        "\n",
        "      Please monitor the pipeline for further updates.\n",
        "      '''\n",
        "\n",
        "      try:\n",
        "          # Set up the SMTP server\n",
        "          server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "          server.starttls()  # Secure the connection\n",
        "          server.login(sender_email, password)\n",
        "\n",
        "          # Send email to each receiver\n",
        "          for receiver_email in receiver_emails:\n",
        "              # Create a fresh message for each recipient\n",
        "              message = MIMEMultipart()\n",
        "              message['From'] = sender_email\n",
        "              message['To'] = receiver_email\n",
        "              message['Subject'] = subject\n",
        "              message.attach(MIMEText(body, 'plain'))\n",
        "\n",
        "              # Send the email\n",
        "              server.sendmail(sender_email, receiver_email, message.as_string())\n",
        "\n",
        "      except Exception as e:\n",
        "          pass\n",
        "      finally:\n",
        "          server.quit()\n",
        "\n",
        "  # Function to send failure email\n",
        "  def send_failure_email(error_message):\n",
        "      sender_email = \"sucessemailtrigger@gmail.com\"\n",
        "      password = \"jomnpxbfunwjgitb\"\n",
        "      receiver_emails = [\"hegde.anir@northeastern.edu\",\n",
        "                         \"nenavath.r@northeastern.edu\",\n",
        "                         \"pandey.raj@northeastern.edu\",\n",
        "                         \"khatri.say@northeastern.edu\",\n",
        "                         \"singh.arc@northeastern.edu\",\n",
        "                         \"goparaju.v@northeastern.edu\"]\n",
        "\n",
        "      # Create the email content\n",
        "      subject = '[Kubeflow Pipeline]'\n",
        "      body = f'Hi team,\\nModel training has failed!.\\nError Details: {error_message}'\n",
        "\n",
        "      try:\n",
        "          # Set up the SMTP server\n",
        "          server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "          server.starttls()  # Secure the connection\n",
        "          server.login(sender_email, password)\n",
        "\n",
        "          # Send email to each receiver\n",
        "          for receiver_email in receiver_emails:\n",
        "              # Create a fresh message for each recipient\n",
        "              message = MIMEMultipart()\n",
        "              message['From'] = sender_email\n",
        "              message['To'] = receiver_email\n",
        "              message['Subject'] = subject\n",
        "              message.attach(MIMEText(body, 'plain'))\n",
        "\n",
        "              # Send the email\n",
        "              server.sendmail(sender_email, receiver_email, message.as_string())\n",
        "\n",
        "      except Exception as e:\n",
        "          pass\n",
        "      finally:\n",
        "          server.quit()\n",
        "  try:\n",
        "    bqclient = bigquery.Client(project=project_id, location=location)\n",
        "\n",
        "    QUERY = f'''select * from `bilingualcomplaint-system.MLOps`.get_dataset_by_complaint_year_interval({start_year}, {end_year}) limit {limit}'''\n",
        "    query_job = bqclient.query(QUERY)  # API request\n",
        "    rows = query_job.result()  # Waits for query to finish\n",
        "    data = rows.to_dataframe()\n",
        "    # Selecting the necessary features and labels\n",
        "    data_features = data[[feature_name, label_name]]\n",
        "\n",
        "    # Initial split\n",
        "    train, val = train_test_split(data_features, test_size=testset_size, random_state=42)\n",
        "\n",
        "    # Identify labels in training set\n",
        "    train_labels = set(train[label_name])\n",
        "\n",
        "    # Filter validation set to remove rows with labels not in training set\n",
        "    val = val[val[label_name].isin(train_labels)]\n",
        "\n",
        "    # Reset indices and save\n",
        "    train.reset_index(drop=True, inplace=True)\n",
        "    val.reset_index(drop=True, inplace=True)\n",
        "    train.to_pickle(train_data.path)\n",
        "    val.to_pickle(val_data.path)\n",
        "\n",
        "    # Track the end time and calculate duration\n",
        "    end_time = datetime.now()\n",
        "    duration = (end_time - start_time).total_seconds() / 60  # Duration in minutes\n",
        "\n",
        "    # Send success email once the data is processed\n",
        "    send_success_email()\n",
        "    # Send the Slack message with execution details\n",
        "    send_slack_message(\n",
        "        component_name=\"Getting Data Component\",\n",
        "        execution_date=end_time.strftime('%Y-%m-%d'),\n",
        "        execution_time=end_time.strftime('%H:%M:%S'),\n",
        "        duration=round(duration, 2)  # Round duration to 2 decimal places\n",
        "    )\n",
        "\n",
        "  except Exception as e:\n",
        "      # Send failure email if there's an error\n",
        "      error_message = str(e)\n",
        "      send_failure_email(error_message)\n",
        "      send_slack_message(\n",
        "          component_name=\"Model Training Component Failed\",\n",
        "          execution_date=datetime.now().strftime('%Y-%m-%d'),\n",
        "          execution_time=datetime.now().strftime('%H:%M:%S'),\n",
        "          duration=0  # If failed, duration is 0\n",
        "      )\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Second Component - Training Script\n",
        "#Training Component\n",
        "@component(\n",
        "    base_image=\"python:3.10-slim\",\n",
        "    packages_to_install=[\n",
        "        'google-cloud-storage==2.18.2',\n",
        "        'pandas==1.5.3',\n",
        "        'numpy==1.23.5',\n",
        "        'scikit-learn==1.2.2',\n",
        "        'xgboost==1.6.1'\n",
        "    ]\n",
        ")\n",
        "def train_xgboost_model(\n",
        "    train_data: Input[Dataset],\n",
        "    feature_name: str,\n",
        "    label_name: str,\n",
        "    model: Output[Model],\n",
        "    vectorizer_output: Output[Artifact],\n",
        "    label_encoder_output: Output[Artifact]  # New output for the LabelEncoder\n",
        "):\n",
        "    import pandas as pd\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    import xgboost as xgb\n",
        "    import os\n",
        "    import pickle\n",
        "    import requests\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Track the start time of the component execution\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    # Function to send custom Slack message with Kubeflow component details\n",
        "    def send_slack_message(component_name, execution_date, execution_time, duration):\n",
        "        SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'  # Replace with your Slack webhook URL\n",
        "        message = {\n",
        "            \"attachments\": [\n",
        "                {\n",
        "                    \"color\": \"#36a64f\",  # Green color for success\n",
        "                    \"pretext\": \":large_green_circle: Kubeflow Component Success Alert\",\n",
        "                    \"fields\": [\n",
        "                        {\n",
        "                            \"title\": \"Component Name\",\n",
        "                            \"value\": component_name,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Execution Date\",\n",
        "                            \"value\": execution_date,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Execution Time\",\n",
        "                            \"value\": execution_time,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Duration\",\n",
        "                            \"value\": f\"{duration} minutes\",\n",
        "                            \"short\": True\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(SLACK_WEBHOOK_URL, json=message)\n",
        "            response.raise_for_status()  # Check for request errors\n",
        "            pass\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            pass\n",
        "    try:\n",
        "      # Load dataset from the train_data input artifact\n",
        "      data = pd.read_pickle(train_data.path)\n",
        "      X = data[feature_name].fillna(\"\")\n",
        "      y = data[label_name].fillna(\"\")\n",
        "\n",
        "      # Encode the target labels\n",
        "      label_encoder = LabelEncoder()\n",
        "      y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "      # Initialize and fit TF-IDF vectorizer\n",
        "      tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "      X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "      # Define and train XGBoost model\n",
        "      xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "      xgb_model.fit(X_tfidf, y_encoded)\n",
        "\n",
        "      # Save the model directly into a \"model\" folder\n",
        "      model_directory = model.path\n",
        "      os.makedirs(model_directory, exist_ok=True)\n",
        "      model_output_path = os.path.join(model_directory, \"model.bst\")\n",
        "      xgb_model.save_model(model_output_path)\n",
        "\n",
        "      # Save the vectorizer as 'tfidf_vectorizer.pkl' directly to the vectorizer_output path\n",
        "      vectorizer_output_path = f\"{vectorizer_output.path}.pkl\"\n",
        "      with open(vectorizer_output_path, 'wb') as f:\n",
        "          pickle.dump(tfidf_vectorizer, f)\n",
        "\n",
        "      # Save the label encoder as 'label_encoder.pkl'\n",
        "      label_encoder_output_path = f\"{label_encoder_output.path}.pkl\"\n",
        "      with open(label_encoder_output_path, 'wb') as f:\n",
        "          pickle.dump(label_encoder, f)\n",
        "\n",
        "      # Track the end time and calculate duration\n",
        "      end_time = datetime.now()\n",
        "      duration = (end_time - start_time).total_seconds() / 60  # Duration in minutes\n",
        "\n",
        "      # Send the Slack message with execution details\n",
        "      send_slack_message(\n",
        "          component_name=\"Model Training Component\",\n",
        "          execution_date=end_time.strftime('%Y-%m-%d'),\n",
        "          execution_time=end_time.strftime('%H:%M:%S'),\n",
        "          duration=round(duration, 2)  # Round duration to 2 decimal places\n",
        "      )\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = str(e)\n",
        "        send_slack_message(\n",
        "            component_name=\"Model Training Component\",\n",
        "            execution_date=datetime.now().strftime('%Y-%m-%d'),\n",
        "            execution_time=datetime.now().strftime('%H:%M:%S'),\n",
        "            duration=0  # If failed, duration is 0\n",
        "        )\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Testing Component\n",
        "@component(\n",
        "    base_image=\"python:3.10-slim\",\n",
        "    packages_to_install=[\n",
        "        'google-cloud-storage==2.18.2',\n",
        "        'pandas==1.5.3',\n",
        "        'numpy==1.23.5',\n",
        "        'scikit-learn==1.2.2',\n",
        "        'xgboost==1.6.1',\n",
        "        'google-cloud-aiplatform==1.18.3'\n",
        "        ]\n",
        ")\n",
        "def test_xgboost_model(\n",
        "    val_data: Input[Dataset],\n",
        "    model_input: Input[Model],\n",
        "    vectorizer_input: Input[Artifact],\n",
        "    label_encoder_input: Input[Artifact],  # New input for the LabelEncoder\n",
        "    feature_name: str,\n",
        "    label_name: str\n",
        ") -> float:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "    import pickle\n",
        "    import xgboost as xgb\n",
        "    import os\n",
        "    import time\n",
        "    import requests\n",
        "    from datetime import datetime\n",
        "    import google.cloud.aiplatform as aiplatform\n",
        "\n",
        "\n",
        "    # Function to send custom Slack message with Kubeflow component details\n",
        "    def send_slack_message(component_name, execution_date, execution_time, duration, f1_score=None, precision=None, recall=None):\n",
        "        SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'\n",
        "        message = {\n",
        "            \"attachments\": [\n",
        "                {\n",
        "                    \"color\": \"#36a64f\",  # Green color for success\n",
        "                    \"pretext\": \":large_green_circle: Kubeflow Component Success Alert\",\n",
        "                    \"fields\": [\n",
        "                        {\n",
        "                            \"title\": \"Component Name\",\n",
        "                            \"value\": component_name,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Execution Date\",\n",
        "                            \"value\": execution_date,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Execution Time\",\n",
        "                            \"value\": execution_time,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Duration\",\n",
        "                            \"value\": f\"{duration} minutes\",\n",
        "                            \"short\": True\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        if f1_score is not None:\n",
        "            message[\"attachments\"][0][\"fields\"].append({\n",
        "                \"title\": \"Validation F1 Score\",\n",
        "                \"value\": f\"{f1_score:.4f}\",\n",
        "                \"short\": True\n",
        "            })\n",
        "\n",
        "        if precision is not None:\n",
        "            message[\"attachments\"][0][\"fields\"].append({\n",
        "                \"title\": \"Validation Precision\",\n",
        "                \"value\": f\"{precision:.4f}\",\n",
        "                \"short\": True\n",
        "            })\n",
        "\n",
        "        if recall is not None:\n",
        "            message[\"attachments\"][0][\"fields\"].append({\n",
        "                \"title\": \"Validation Recall\",\n",
        "                \"value\": f\"{recall:.4f}\",\n",
        "                \"short\": True\n",
        "            })\n",
        "\n",
        "        try:\n",
        "            response = requests.post(SLACK_WEBHOOK_URL, json=message)\n",
        "            response.raise_for_status()  # Check for request errors\n",
        "        except requests.exceptions.RequestException as e:\n",
        "          pass\n",
        "\n",
        "    try:\n",
        "      # Track the start time of the component execution\n",
        "      start_time = datetime.now()\n",
        "\n",
        "      # Load the trained model\n",
        "      model_path = os.path.join(model_input.path, \"model.bst\")\n",
        "      xgb_model = xgb.XGBClassifier()\n",
        "      xgb_model.load_model(model_path)\n",
        "\n",
        "      # Load the TF-IDF vectorizer\n",
        "      vectorizer_path = f\"{vectorizer_input.path}.pkl\"\n",
        "      with open(vectorizer_path, 'rb') as f:\n",
        "          tfidf_vectorizer = pickle.load(f)\n",
        "\n",
        "      # Load the LabelEncoder\n",
        "      label_encoder_path = f\"{label_encoder_input.path}.pkl\"\n",
        "      with open(label_encoder_path, 'rb') as f:\n",
        "          label_encoder = pickle.load(f)\n",
        "\n",
        "      # Load validation dataset from val_data input artifact\n",
        "      data = pd.read_pickle(val_data.path)\n",
        "      X_val = data[feature_name].fillna(\"\")\n",
        "      y_val = data[label_name].fillna(\"\")\n",
        "\n",
        "      # Transform validation data with vectorizer\n",
        "      X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "\n",
        "      # Encode validation labels using the loaded label encoder\n",
        "      y_val_encoded = label_encoder.transform(y_val)\n",
        "\n",
        "      # Make predictions\n",
        "      y_pred_encoded = xgb_model.predict(X_val_tfidf)\n",
        "\n",
        "      # Decode predictions back to the original label format\n",
        "      y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
        "\n",
        "\n",
        "      aiplatform.init(project=\"bilingualcomplaint-system\", location=\"us-east1\", experiment='experiment-demo')\n",
        "      run = aiplatform.start_run(\"run-{}\".format(int(time.time())))\n",
        "\n",
        "      # Calculate metrics\n",
        "      f1 = f1_score(y_val_encoded, y_pred_encoded, average=\"macro\")\n",
        "      precision = precision_score(y_val_encoded, y_pred_encoded, average=\"macro\")\n",
        "      recall = recall_score(y_val_encoded, y_pred_encoded, average=\"macro\")\n",
        "      metrics = {}\n",
        "      metrics[\"f1\"] = f1\n",
        "      metrics[\"precision\"] = precision\n",
        "      metrics[\"recall\"] = recall\n",
        "\n",
        "      aiplatform.log_metrics(metrics)\n",
        "      run.end_run()\n",
        "\n",
        "      print(f\"Validation F1 Score: {f1:.4f}\")\n",
        "      print(f\"Validation Precision: {precision:.4f}\")\n",
        "      print(f\"Validation Recall: {recall:.4f}\")\n",
        "\n",
        "      # Track the end time and calculate duration\n",
        "      end_time = datetime.now()\n",
        "      duration = (end_time - start_time).total_seconds() / 60  # Duration in minutes\n",
        "\n",
        "      # Send the Slack message with execution details and metrics\n",
        "      send_slack_message(\n",
        "          component_name=\"Model Testing Component\",\n",
        "          execution_date=end_time.strftime('%Y-%m-%d'),\n",
        "          execution_time=end_time.strftime('%H:%M:%S'),\n",
        "          duration=round(duration, 2),\n",
        "          f1_score=f1,\n",
        "          precision=precision,\n",
        "          recall=recall\n",
        "      )\n",
        "\n",
        "    except Exception as e:\n",
        "      error_message = str(e)\n",
        "      print(f\"Error during model testing: {error_message}\")\n",
        "      send_slack_message(\n",
        "          component_name=\"Model Testing Component Failed\",\n",
        "          execution_date=datetime.now().strftime('%Y-%m-%d'),\n",
        "          execution_time=datetime.now().strftime('%H:%M:%S'),\n",
        "          duration= 0  # If failed, duration is 0\n",
        "      )\n",
        "      raise e\n",
        "\n",
        "\n",
        "    # # 2nd try-except block for **BigQuery Insertion**\n",
        "    # try:\n",
        "    #     project_id = \"bilingualcomplaint-system\"\n",
        "    #     current_timestamp = datetime.utcnow().isoformat()  # Get the current timestamp\n",
        "\n",
        "    #     # if not f1:\n",
        "    #     #     f1_score = 0.0\n",
        "\n",
        "    #     rows_to_insert = [\n",
        "    #         {\n",
        "    #             \"last_training_timestamp\": current_timestamp,\n",
        "    #             \"f1_score\": float(f1)\n",
        "    #         }\n",
        "    #     ]\n",
        "\n",
        "    #     # Initialize BigQuery client\n",
        "    #     bqclient = bigquery.Client(project=project_id)\n",
        "\n",
        "    #     # Define the BigQuery table name\n",
        "    #     metadata_table = f\"{project_id}.MLOps.metrics\"\n",
        "\n",
        "    #     # Insert metrics into BigQuery table\n",
        "    #     errors = bqclient.insert_rows_json(metadata_table, rows_to_insert)\n",
        "\n",
        "    #     if errors:\n",
        "    #         print(f\"Error inserting data: {errors}\")\n",
        "    #     else:\n",
        "    #         print(f\"Data inserted successfully into {metadata_table}\")\n",
        "\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error during BigQuery insertion: {str(e)}\")\n",
        "    #     raise e  # Re-raise to propagate the failure\n",
        "\n",
        "    return f1\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# register\n",
        "@component(\n",
        "    packages_to_install=[\"google-cloud-aiplatform\", \"google-auth\"]\n",
        ")\n",
        "def model_registration(\n",
        "    model_output: Input[Model],\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    model_display_name: str,\n",
        "    model: Output[Model]\n",
        "):\n",
        "    from google.cloud import aiplatform\n",
        "\n",
        "    # Initialize Vertex AI SDK\n",
        "    aiplatform.init(project=project_id, location=location)\n",
        "\n",
        "    # Use the URI directly without appending \"/model\"\n",
        "    model_uri = model_output.uri\n",
        "\n",
        "    # Check if a model with the same display name already exists\n",
        "    existing_models = aiplatform.Model.list(\n",
        "        filter=f'display_name=\"{model_display_name}\"',\n",
        "        order_by='create_time desc',\n",
        "        project=project_id,\n",
        "        location=location\n",
        "    )\n",
        "\n",
        "    if existing_models:\n",
        "        # Get the first existing model's resource name (ID) to add a new version under it\n",
        "        parent_model = existing_models[0]\n",
        "        model_id = parent_model.resource_name\n",
        "\n",
        "        # Register the model under the existing model ID\n",
        "        registered_model = aiplatform.Model.upload(\n",
        "            display_name=model_display_name,\n",
        "            artifact_uri=model_uri,\n",
        "            serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-5:latest\",\n",
        "            parent_model=model_id  # Use the existing model's ID to create a new version\n",
        "        )\n",
        "    else:\n",
        "        # No existing model, create a new one\n",
        "        registered_model = aiplatform.Model.upload(\n",
        "            display_name=model_display_name,\n",
        "            artifact_uri=model_uri,\n",
        "            serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-5:latest\"\n",
        "        )\n",
        "\n",
        "    # Output the model resource name\n",
        "    model.uri = registered_model.resource_name\n",
        "\n",
        "\n",
        "\n",
        "@component(\n",
        "    packages_to_install=[\"google-cloud-aiplatform\"]\n",
        ")\n",
        "def model_deployment(\n",
        "    model: Input[Model],\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    endpoint_display_name: str,\n",
        "    deployed_model_display_name: str,\n",
        "    endpoint: Output[Artifact]\n",
        "):\n",
        "    import time\n",
        "    from google.cloud import aiplatform, bigquery\n",
        "    import requests\n",
        "    from datetime import datetime\n",
        "    from email.mime.multipart import MIMEMultipart\n",
        "    from email.mime.text import MIMEText\n",
        "    import smtplib\n",
        "\n",
        "    # Function to send success email\n",
        "    def send_success_email():\n",
        "        sender_email = \"sucessemailtrigger@gmail.com\"\n",
        "        password = \"jomnpxbfunwjgitb\"\n",
        "        receiver_emails = [\"hegde.anir@northeastern.edu\",\n",
        "                          \"nenavath.r@northeastern.edu\",\n",
        "                          \"pandey.raj@northeastern.edu\",\n",
        "                          \"khatri.say@northeastern.edu\",\n",
        "                          \"singh.arc@northeastern.edu\",\n",
        "                          \"goparaju.v@northeastern.edu\"]\n",
        "\n",
        "\n",
        "        # Current time for logging purposes\n",
        "        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        # Create the email content\n",
        "        subject = '[Kubeflow Pipeline] - Completed'\n",
        "        body = f'''Hi team,\n",
        "\n",
        "        Model has been deployed!\n",
        "\n",
        "        Details:\n",
        "        - Start Time: {current_time}\n",
        "\n",
        "\n",
        "        '''\n",
        "\n",
        "        try:\n",
        "            # Set up the SMTP server\n",
        "            server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "            server.starttls()  # Secure the connection\n",
        "            server.login(sender_email, password)\n",
        "\n",
        "            # Send email to each receiver\n",
        "            for receiver_email in receiver_emails:\n",
        "                # Create a fresh message for each recipient\n",
        "                message = MIMEMultipart()\n",
        "                message['From'] = sender_email\n",
        "                message['To'] = receiver_email\n",
        "                message['Subject'] = subject\n",
        "                message.attach(MIMEText(body, 'plain'))\n",
        "\n",
        "                # Send the email\n",
        "                server.sendmail(sender_email, receiver_email, message.as_string())\n",
        "\n",
        "        except Exception as e:\n",
        "            pass\n",
        "        finally:\n",
        "            server.quit()\n",
        "\n",
        "    # Function to send failure email\n",
        "    def send_failure_email(error_message):\n",
        "        sender_email = \"sucessemailtrigger@gmail.com\"\n",
        "        password = \"jomnpxbfunwjgitb\"\n",
        "        receiver_emails = [\"hegde.anir@northeastern.edu\",\n",
        "                          \"nenavath.r@northeastern.edu\",\n",
        "                          \"pandey.raj@northeastern.edu\",\n",
        "                          \"khatri.say@northeastern.edu\",\n",
        "                          \"singh.arc@northeastern.edu\",\n",
        "                          \"goparaju.v@northeastern.edu\"]\n",
        "\n",
        "        # Create the email content\n",
        "        subject = '[Kubeflow Pipeline]'\n",
        "        body = f'Hi team,\\nModel deployment has failed!.\\nError Details: {error_message}'\n",
        "\n",
        "        try:\n",
        "            # Set up the SMTP server\n",
        "            server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "            server.starttls()  # Secure the connection\n",
        "            server.login(sender_email, password)\n",
        "\n",
        "            # Send email to each receiver\n",
        "            for receiver_email in receiver_emails:\n",
        "                # Create a fresh message for each recipient\n",
        "                message = MIMEMultipart()\n",
        "                message['From'] = sender_email\n",
        "                message['To'] = receiver_email\n",
        "                message['Subject'] = subject\n",
        "                message.attach(MIMEText(body, 'plain'))\n",
        "\n",
        "                # Send the email\n",
        "                server.sendmail(sender_email, receiver_email, message.as_string())\n",
        "\n",
        "        except Exception as e:\n",
        "            pass\n",
        "        finally:\n",
        "            server.quit()\n",
        "\n",
        "    # Function to send custom Slack message with Kubeflow component details\n",
        "    def send_slack_message(component_name, execution_date, execution_time, duration, endpoint_name):\n",
        "        SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'  # Replace with your Slack webhook URL\n",
        "        message = {\n",
        "            \"attachments\": [\n",
        "                {\n",
        "                    \"color\": \"#36a64f\",  # Green color for success\n",
        "                    \"pretext\": \":large_green_circle: Kubeflow Component Alert\",\n",
        "                    \"fields\": [\n",
        "                        {\n",
        "                            \"title\": \"Component Name\",\n",
        "                            \"value\": component_name,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Execution Date\",\n",
        "                            \"value\": execution_date,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Execution Time\",\n",
        "                            \"value\": execution_time,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Duration\",\n",
        "                            \"value\": f\"{duration} minutes\",\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Deployed Endpoint\",\n",
        "                            \"value\": endpoint_name,\n",
        "                            \"short\": True\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(SLACK_WEBHOOK_URL, json=message)\n",
        "            response.raise_for_status()  # Check for request errors\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            pass\n",
        "\n",
        "    # Track the start time of the component execution\n",
        "    start_time = datetime.now()\n",
        "    # Delay to allow for model registration completion\n",
        "    time.sleep(35)\n",
        "\n",
        "    try:\n",
        "      # Delay to allow for model registration completion\n",
        "      time.sleep(30)  # Wait for 30 seconds\n",
        "\n",
        "      # Initialize Vertex AI SDK\n",
        "      aiplatform.init(project=project_id, location=location)\n",
        "\n",
        "      # Retrieve the model using its resource name (model.uri should be in the format projects/PROJECT_ID/locations/LOCATION/models/MODEL_ID)\n",
        "      deployed_model = aiplatform.Model(model.uri)\n",
        "\n",
        "      # Create or get an existing endpoint\n",
        "      endpoints = aiplatform.Endpoint.list(\n",
        "          filter=f'display_name=\"{endpoint_display_name}\"',\n",
        "          order_by='create_time desc',\n",
        "          project=project_id,\n",
        "          location=location\n",
        "      )\n",
        "      if endpoints:\n",
        "          endpoint_obj = endpoints[0]\n",
        "      else:\n",
        "          endpoint_obj = aiplatform.Endpoint.create(display_name=endpoint_display_name)\n",
        "\n",
        "      # Deploy the model to the endpoint with 100% traffic\n",
        "      deployed_model_resource = endpoint_obj.deploy(\n",
        "          model=deployed_model,\n",
        "          deployed_model_display_name=deployed_model_display_name,\n",
        "          machine_type=\"n1-standard-4\",\n",
        "          traffic_split={\"0\": 100},  # Assign 100% traffic to the new deployment\n",
        "      )\n",
        "\n",
        "      # Ensure that deployed_model_resource is not None before accessing its ID\n",
        "      if deployed_model_resource is not None and hasattr(deployed_model_resource, \"id\"):\n",
        "          # Retrieve the current traffic allocation and set traffic of old versions to 0%\n",
        "          traffic_split = {deployed_model_resource.id: 100}  # New model gets 100% traffic\n",
        "          for deployed_model_id in endpoint_obj.traffic_split.keys():\n",
        "              if deployed_model_id != deployed_model_resource.id:\n",
        "                  traffic_split[deployed_model_id] = 0  # Set old versions to 0% traffic\n",
        "\n",
        "          # Update the endpoint's traffic split\n",
        "          endpoint_obj.update(traffic_split=traffic_split)\n",
        "      else:\n",
        "          print(\"Warning: Deployed model resource is None or lacks an ID attribute.\")\n",
        "\n",
        "      # Output the endpoint resource name\n",
        "      endpoint.uri = endpoint_obj.resource_name\n",
        "\n",
        "      # Track the end time and calculate duration\n",
        "      end_time = datetime.now()\n",
        "      duration = (end_time - start_time).total_seconds() / 60  # Duration in minutes\n",
        "\n",
        "      send_success_email()\n",
        "      # Send Slack and success email notifications\n",
        "      send_slack_message(\n",
        "          component_name=\"Model Deployment Component\",\n",
        "          execution_date=end_time.strftime('%Y-%m-%d'),\n",
        "          execution_time=end_time.strftime('%H:%M:%S'),\n",
        "          duration=round(duration, 2),\n",
        "          endpoint_name=endpoint_display_name\n",
        "      )\n",
        "\n",
        "    except Exception as e:\n",
        "      # Send failure Slack message and email in case of an error\n",
        "      error_message = str(e)\n",
        "      send_failure_email(error_message)\n",
        "      send_slack_message(\n",
        "          component_name=\"Model Deployment Component Failed\",\n",
        "          execution_date=datetime.now().strftime('%Y-%m-%d'),\n",
        "          execution_time=datetime.now().strftime('%H:%M:%S'),\n",
        "          duration=0,  # If failed, duration is 0\n",
        "          endpoint_name=endpoint_display_name\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "    # Separate block for BigQuery insertion (not part of the try block for deployment)\n",
        "    try:\n",
        "        project_id = \"bilingualcomplaint-system\"\n",
        "        metadata_table = \"bilingualcomplaint-system.MLOps.model_training_metadata\"\n",
        "        preprocessed_data_table = \"bilingualcomplaint-system.MLOps.preprocessed_data\"\n",
        "        # Initialize BigQuery client\n",
        "        bqclient = bigquery.Client(project=project_id)\n",
        "\n",
        "        # Define metadata insertion details\n",
        "        current_timestamp = datetime.utcnow().isoformat()\n",
        "\n",
        "        # Query to get the record count from the preprocessed_data table\n",
        "        query = f\"SELECT COUNT(*) AS record_count FROM `{preprocessed_data_table}`\"\n",
        "        query_job = bqclient.query(query)\n",
        "\n",
        "        # Fetch the query result and process it directly\n",
        "        result = query_job.result()  # This returns a list of Row objects\n",
        "        record_count = None\n",
        "\n",
        "        # Iterate over the results and extract the record count\n",
        "        for row in result:\n",
        "            record_count = int(row[\"record_count\"])\n",
        "\n",
        "        if record_count is None:\n",
        "            raise ValueError(\"No record count returned from BigQuery query.\")\n",
        "\n",
        "        # Prepare data for metadata insertion\n",
        "        rows_to_insert = [\n",
        "            {\n",
        "                \"last_training_timestamp\": current_timestamp,\n",
        "                \"record_count\": record_count\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Insert metadata into BigQuery table\n",
        "        metadata_table = f\"{project_id}.MLOps.model_training_metadata\"\n",
        "        errors = bqclient.insert_rows_json(metadata_table, rows_to_insert)\n",
        "        print(\"hi\")\n",
        "\n",
        "        if errors:\n",
        "            print(f\"Failed to insert metadata: {errors}\")\n",
        "        else:\n",
        "            print(f\"Metadata inserted successfully into {metadata_table}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error inserting metadata into BigQuery: {str(e)}\")\n",
        "#-------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBLI-zuC1UPl",
        "outputId": "df95eee7-aea2-4c49-e5c3-408c0f460a98"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KFB version: 2.10.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/kfp/dsl/component_decorator.py:126: FutureWarning: The default base_image used by the @dsl.component decorator will switch from 'python:3.9' to 'python:3.10' on Oct 1, 2025. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.10.\n",
            "  return component_factory.create_component_from_func(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------------------------------------------------------------------------------------------\n",
        "# Bias detection component\n",
        "\n",
        "\n",
        "@component(\n",
        "    base_image=\"python:3.10-slim\",\n",
        "    packages_to_install=[\n",
        "        'google-cloud-storage==2.18.2',\n",
        "        'pandas==1.5.3',\n",
        "        'numpy==1.23.5',\n",
        "        'scikit-learn==1.2.2',\n",
        "        'xgboost==1.6.1',\n",
        "        'fairlearn==0.8.0'\n",
        "    ]\n",
        ")\n",
        "def bias_detection(\n",
        "    train_data: Input[Dataset],\n",
        "    model_input: Input[Model],\n",
        "    vectorizer_input: Input[Artifact],\n",
        "    label_encoder_input: Input[Artifact],\n",
        "    feature_name: str,\n",
        "    label_name: str,\n",
        "):\n",
        "    import pandas as pd\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    from fairlearn.metrics import MetricFrame\n",
        "    import xgboost as xgb\n",
        "    import os\n",
        "    import pickle\n",
        "    import requests\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Function to send custom Slack message with Kubeflow component details\n",
        "    def send_slack_message(component_name, execution_date, execution_time, duration, alerts=None, no_bias_message=False, slice_results=None):\n",
        "        SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'\n",
        "        message = {\n",
        "            \"attachments\": [\n",
        "                {\n",
        "                    \"color\": \"#36a64f\",  # Green color for success\n",
        "                    \"pretext\": \":large_green_circle: Kubeflow Bias Component Check Alert\",\n",
        "                    \"fields\": [\n",
        "                        {\n",
        "                            \"title\": \"Component Name\",\n",
        "                            \"value\": component_name,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Execution Date\",\n",
        "                            \"value\": execution_date,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Execution Time\",\n",
        "                            \"value\": execution_time,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Duration\",\n",
        "                            \"value\": f\"{duration} minutes\",\n",
        "                            \"short\": True\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        if alerts:\n",
        "            message[\"attachments\"][0][\"pretext\"] = \":warning: Bias Detected\"\n",
        "            for alert in alerts:\n",
        "                message[\"attachments\"][0][\"fields\"].append({\n",
        "                    \"title\": \"Bias Alert\",\n",
        "                    \"value\": alert,\n",
        "                    \"short\": False\n",
        "                })\n",
        "\n",
        "        elif no_bias_message:\n",
        "            message[\"attachments\"][0][\"pretext\"] = \":white_check_mark: No Bias Detected\"\n",
        "            message[\"attachments\"][0][\"fields\"].append({\n",
        "                \"title\": \"Bias Check Status\",\n",
        "                \"value\": \"Everything is fine. No bias detected across any slices.\",\n",
        "                \"short\": False\n",
        "            })\n",
        "\n",
        "        if slice_results is not None:\n",
        "            message[\"attachments\"][0][\"fields\"].append({\n",
        "                \"title\": \"Slice Accuracy Results\",\n",
        "                \"value\": slice_results,\n",
        "                \"short\": False\n",
        "            })\n",
        "\n",
        "        try:\n",
        "            response = requests.post(SLACK_WEBHOOK_URL, json=message)\n",
        "            response.raise_for_status()  # Check for request errors\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            pass\n",
        "\n",
        "    try:\n",
        "        sensitive_features = 'product'  # We are now only checking for the 'product' feature\n",
        "        bias_threshold = 0.99  # Example threshold for bias detection\n",
        "        # Track the start time of the component execution\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        # Load the trained model\n",
        "        model_path = os.path.join(model_input.path, \"model.bst\")\n",
        "        xgb_model = xgb.XGBClassifier()\n",
        "        xgb_model.load_model(model_path)\n",
        "\n",
        "        # Load the TF-IDF vectorizer\n",
        "        vectorizer_path = f\"{vectorizer_input.path}.pkl\"\n",
        "        with open(vectorizer_path, 'rb') as f:\n",
        "            tfidf_vectorizer = pickle.load(f)\n",
        "\n",
        "        # Load the LabelEncoder\n",
        "        label_encoder_path = f\"{label_encoder_input.path}.pkl\"\n",
        "        with open(label_encoder_path, 'rb') as f:\n",
        "            label_encoder = pickle.load(f)\n",
        "\n",
        "        # Load the dataset from train_data input artifact\n",
        "        data = pd.read_pickle(train_data.path)\n",
        "        X = data[feature_name].fillna(\"\")  # Extract the features\n",
        "        y = data[label_name].fillna(\"\")   # Extract the label\n",
        "\n",
        "        # Transform validation data with vectorizer\n",
        "        X_tfidf = tfidf_vectorizer.transform(X)\n",
        "\n",
        "        # Make predictions using the loaded model\n",
        "        y_pred_encoded = xgb_model.predict(X_tfidf)\n",
        "\n",
        "        # Decode predictions back to the original label format\n",
        "        y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
        "\n",
        "        # Bias detection using Fairlearn's MetricFrame\n",
        "        metric_frame = MetricFrame(\n",
        "            metrics=accuracy_score,\n",
        "            y_true=y,\n",
        "            y_pred=y_pred,\n",
        "            sensitive_features=data[sensitive_features]  # We are checking for the 'product' feature\n",
        "        )\n",
        "\n",
        "        # Calculate slice-specific metrics\n",
        "        slice_metrics = metric_frame.by_group\n",
        "        valid_slices = slice_metrics.dropna()\n",
        "\n",
        "        # Convert the slice metrics into a readable format for Slack\n",
        "        slice_results = valid_slices.reset_index()\n",
        "        slice_results.columns = ['Product', 'Accuracy']  # Ensuring column names are correct for Slack\n",
        "        slice_results_str = slice_results.to_string(index=False)\n",
        "\n",
        "        # Function to check for bias and trigger an alert if threshold is crossed\n",
        "        def check_for_bias(slice_metrics, threshold):\n",
        "            alert_flag = False\n",
        "            alerts = []\n",
        "\n",
        "            for slice_name, accuracy in slice_metrics.items():\n",
        "                if accuracy < threshold:\n",
        "                    alert_flag = True\n",
        "                    alerts.append(f\"Bias Alert: Accuracy for slice '{slice_name}' is below threshold: {accuracy:.2f}. We are removing these classes for mitigation.\")\n",
        "\n",
        "            if alert_flag:\n",
        "                return alerts\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "        # Check for bias and trigger alerts\n",
        "        bias_alerts = check_for_bias(valid_slices, bias_threshold)\n",
        "\n",
        "        # Track the end time and calculate duration\n",
        "        end_time = datetime.now()\n",
        "        duration = (end_time - start_time).total_seconds() / 60  # Duration in minutes\n",
        "\n",
        "        # Send Slack message with execution details and bias alerts if any\n",
        "        if bias_alerts:\n",
        "            send_slack_message(\n",
        "                component_name=\"Bias Detection Component\",\n",
        "                execution_date=end_time.strftime('%Y-%m-%d'),\n",
        "                execution_time=end_time.strftime('%H:%M:%S'),\n",
        "                duration=round(duration, 2),\n",
        "                alerts=bias_alerts,\n",
        "                slice_results=slice_results_str  # Include the slice results in the Slack message\n",
        "            )\n",
        "        else:\n",
        "            send_slack_message(\n",
        "                component_name=\"Bias Detection Component\",\n",
        "                execution_date=end_time.strftime('%Y-%m-%d'),\n",
        "                execution_time=end_time.strftime('%H:%M:%S'),\n",
        "                duration=round(duration, 2),\n",
        "                no_bias_message=True\n",
        "            )\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = str(e)\n",
        "        print(f\"Error during bias detection: {error_message}\")\n",
        "        send_slack_message(\n",
        "            component_name=\"Bias Detection Component Failed\",\n",
        "            execution_date=datetime.now().strftime('%Y-%m-%d'),\n",
        "            execution_time=datetime.now().strftime('%H:%M:%S'),\n",
        "            duration=0  # If failed, duration is 0\n",
        "        )\n",
        "        raise e\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "EuC5qM_-kVNO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Second Component - Training Script\n",
        "#Training Component\n",
        "@component(\n",
        "    base_image=\"python:3.10-slim\",\n",
        "    packages_to_install=[\n",
        "        'google-cloud-storage==2.18.2',\n",
        "        'pandas==1.5.3',\n",
        "        'numpy==1.23.5',\n",
        "        'scikit-learn==1.2.2',\n",
        "    ]\n",
        ")\n",
        "def train_naive_bayes_model(\n",
        "    train_data: Input[Dataset],\n",
        "    feature_name: str,\n",
        "    label_name: str,\n",
        "    model: Output[Model],\n",
        "    vectorizer_output: Output[Artifact],\n",
        "    label_encoder_output: Output[Artifact]  # New output for the LabelEncoder\n",
        "):\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.naive_bayes import MultinomialNB\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    import pickle\n",
        "    import requests\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Track the start time of the component execution\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    # Function to send custom Slack message with Kubeflow component details\n",
        "    def send_slack_message(component_name, execution_date, execution_time, duration):\n",
        "        SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'  # Replace with your Slack webhook URL\n",
        "        message = {\n",
        "            \"attachments\": [\n",
        "                {\n",
        "                    \"color\": \"#36a64f\",  # Green color for success\n",
        "                    \"pretext\": \":large_green_circle: Kubeflow Component Success Alert\",\n",
        "                    \"fields\": [\n",
        "                        {\n",
        "                            \"title\": \"Component Name\",\n",
        "                            \"value\": component_name,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Execution Date\",\n",
        "                            \"value\": execution_date,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Execution Time\",\n",
        "                            \"value\": execution_time,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Duration\",\n",
        "                            \"value\": f\"{duration} minutes\",\n",
        "                            \"short\": True\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(SLACK_WEBHOOK_URL, json=message)\n",
        "            response.raise_for_status()  # Check for request errors\n",
        "            pass\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            pass\n",
        "    try:\n",
        "      # Load dataset from the train_data input artifact\n",
        "      data = pd.read_pickle(train_data.path)\n",
        "      X = data[feature_name].fillna(\"\")\n",
        "      y = data[label_name].fillna(\"\")\n",
        "\n",
        "      # Encode the target labels\n",
        "      label_encoder = LabelEncoder()\n",
        "      y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "      # Initialize and fit TF-IDF vectorizer\n",
        "      tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "      X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "      mnb = MultinomialNB(\n",
        "          alpha=0.8,\n",
        "          fit_prior=True,\n",
        "          force_alpha=True\n",
        "          )\n",
        "      mnb.fit(X_tfidf, y_encoded)\n",
        "\n",
        "      # Save the model as 'naive_bayes_model.pkl'\n",
        "      model_path = f\"{model.path}.pkl\"\n",
        "      with open(model_path, 'wb') as f:\n",
        "          pickle.dump(mnb, f)\n",
        "\n",
        "      # Save the vectorizer as 'tfidf_vectorizer.pkl' directly to the vectorizer_output path\n",
        "      vectorizer_output_path = f\"{vectorizer_output.path}.pkl\"\n",
        "      with open(vectorizer_output_path, 'wb') as f:\n",
        "          pickle.dump(tfidf_vectorizer, f)\n",
        "\n",
        "      # Save the label encoder as 'label_encoder.pkl'\n",
        "      label_encoder_output_path = f\"{label_encoder_output.path}.pkl\"\n",
        "      with open(label_encoder_output_path, 'wb') as f:\n",
        "          pickle.dump(label_encoder, f)\n",
        "\n",
        "      # Track the end time and calculate duration\n",
        "      end_time = datetime.now()\n",
        "      duration = (end_time - start_time).total_seconds() / 60  # Duration in minutes\n",
        "\n",
        "      # Send the Slack message with execution details\n",
        "      send_slack_message(\n",
        "          component_name=\"Model Training Component (Naive Bayes Model Training)\",\n",
        "          execution_date=end_time.strftime('%Y-%m-%d'),\n",
        "          execution_time=end_time.strftime('%H:%M:%S'),\n",
        "          duration=round(duration, 2)  # Round duration to 2 decimal places\n",
        "      )\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = str(e)\n",
        "        send_slack_message(\n",
        "            component_name=\"Model Training Component (Naive Bayes Model Training)\",\n",
        "            execution_date=datetime.now().strftime('%Y-%m-%d'),\n",
        "            execution_time=datetime.now().strftime('%H:%M:%S'),\n",
        "            duration=0  # If failed, duration is 0\n",
        "        )\n",
        "\n",
        "@component(\n",
        "    base_image=\"python:3.10-slim\",\n",
        "    packages_to_install=[\n",
        "        'google-cloud-storage==2.18.2',\n",
        "        'pandas==1.5.3',\n",
        "        'numpy==1.23.5',\n",
        "        'scikit-learn==1.2.2',\n",
        "        'google-cloud-aiplatform==1.18.3'\n",
        "    ]\n",
        ")\n",
        "def test_naive_bayes_model(\n",
        "    val_data: Input[Dataset],\n",
        "    model_input: Input[Model],\n",
        "    vectorizer_input: Input[Artifact],\n",
        "    label_encoder_input: Input[Artifact],\n",
        "    feature_name: str,\n",
        "    label_name: str\n",
        ") -> float:\n",
        "    import pandas as pd\n",
        "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "    import pickle\n",
        "    from datetime import datetime\n",
        "    # from google.cloud import bigquery\n",
        "    import requests\n",
        "    import time\n",
        "    import google.cloud.aiplatform as aiplatform\n",
        "\n",
        "    # Function to send custom Slack message with Kubeflow component details\n",
        "    def send_slack_message(component_name, execution_date, execution_time, duration, f1_score=None, precision=None, recall=None):\n",
        "        SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'\n",
        "        message = {\n",
        "            \"attachments\": [\n",
        "                {\n",
        "                    \"color\": \"#36a64f\",  # Green color for success\n",
        "                    \"pretext\": \":large_green_circle: Kubeflow Component Success Alert\",\n",
        "                    \"fields\": [\n",
        "                        {\"title\": \"Component Name\", \"value\": component_name, \"short\": True},\n",
        "                        {\"title\": \"Execution Date\", \"value\": execution_date, \"short\": True},\n",
        "                        {\"title\": \"Execution Time\", \"value\": execution_time, \"short\": True},\n",
        "                        {\"title\": \"Duration\", \"value\": f\"{duration} minutes\", \"short\": True}\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        if f1_score is not None:\n",
        "            message[\"attachments\"][0][\"fields\"].append({\"title\": \"Validation F1 Score\", \"value\": f\"{f1_score:.4f}\", \"short\": True})\n",
        "        if precision is not None:\n",
        "            message[\"attachments\"][0][\"fields\"].append({\"title\": \"Validation Precision\", \"value\": f\"{precision:.4f}\", \"short\": True})\n",
        "        if recall is not None:\n",
        "            message[\"attachments\"][0][\"fields\"].append({\"title\": \"Validation Recall\", \"value\": f\"{recall:.4f}\", \"short\": True})\n",
        "\n",
        "        try:\n",
        "            response = requests.post(SLACK_WEBHOOK_URL, json=message)\n",
        "            response.raise_for_status()  # Check for request errors\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error sending Slack message: {e}\")\n",
        "\n",
        "    try:\n",
        "        # Track the start time of the component execution\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        # Load the trained model\n",
        "        with open(f'{model_input.path}.pkl', 'rb') as f:\n",
        "            mnb = pickle.load(f)\n",
        "\n",
        "        # Load the TF-IDF vectorizer\n",
        "        with open(f'{vectorizer_input.path}.pkl', 'rb') as f:\n",
        "            tfidf_vectorizer = pickle.load(f)\n",
        "\n",
        "        # Load the LabelEncoder\n",
        "        with open(f'{label_encoder_input.path}.pkl', 'rb') as f:\n",
        "            label_encoder = pickle.load(f)\n",
        "\n",
        "        # Load validation dataset from val_data input artifact\n",
        "        data = pd.read_pickle(val_data.path)\n",
        "        X_val = data[feature_name].fillna(\"\")\n",
        "        y_val = data[label_name].fillna(\"\")\n",
        "\n",
        "        # Transform validation data with vectorizer\n",
        "        X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "\n",
        "        # Encode validation labels using the loaded label encoder\n",
        "        y_val_encoded = label_encoder.transform(y_val)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred_encoded = mnb.predict(X_val_tfidf)\n",
        "\n",
        "        # Decode predictions back to the original label format\n",
        "        y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
        "\n",
        "\n",
        "        aiplatform.init(project=\"bilingualcomplaint-system\", location=\"us-east1\", experiment='experiment-demo')\n",
        "        run = aiplatform.start_run(\"run-{}\".format(int(time.time())))\n",
        "\n",
        "        # Calculate metrics\n",
        "        f1 = f1_score(y_val_encoded, y_pred_encoded, average=\"macro\")\n",
        "        precision = precision_score(y_val_encoded, y_pred_encoded, average=\"macro\")\n",
        "        recall = recall_score(y_val_encoded, y_pred_encoded, average=\"macro\")\n",
        "        metrics = {}\n",
        "        metrics[\"f1\"] = f1\n",
        "        metrics[\"precision\"] = precision\n",
        "        metrics[\"recall\"] = recall\n",
        "\n",
        "        aiplatform.log_metrics(metrics)\n",
        "        run.end_run()\n",
        "\n",
        "        print(f\"Validation F1 Score: {f1:.4f}\")\n",
        "        print(f\"Validation Precision: {precision:.4f}\")\n",
        "        print(f\"Validation Recall: {recall:.4f}\")\n",
        "\n",
        "        # Track the end time and calculate duration\n",
        "        end_time = datetime.now()\n",
        "        duration = (end_time - start_time).total_seconds() / 60  # Duration in minutes\n",
        "\n",
        "        # Send the Slack message with execution details and metrics\n",
        "        send_slack_message(\n",
        "            component_name=\"Model Testing Component Testing (Naive Bayes Model Testing)\",\n",
        "            execution_date=end_time.strftime('%Y-%m-%d'),\n",
        "            execution_time=end_time.strftime('%H:%M:%S'),\n",
        "            duration=round(duration, 2),\n",
        "            f1_score=f1,\n",
        "            precision=precision,\n",
        "            recall=recall\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = str(e)\n",
        "        print(f\"Error during model testing: {error_message}\")\n",
        "        send_slack_message(\n",
        "            component_name=\"Model Testing Component Failed (Naive Bayes Model Testing)\",\n",
        "            execution_date=datetime.now().strftime('%Y-%m-%d'),\n",
        "            execution_time=datetime.now().strftime('%H:%M:%S'),\n",
        "            duration=0  # If failed, duration is 0\n",
        "        )\n",
        "        raise e\n",
        "\n",
        "    # # BigQuery Insertion\n",
        "    # try:\n",
        "    #     project_id = \"bilingualcomplaint-system\"\n",
        "    #     current_timestamp = datetime.utcnow().isoformat()  # Get the current timestamp\n",
        "\n",
        "    #     rows_to_insert = [\n",
        "    #         {\n",
        "    #             \"last_training_timestamp\": current_timestamp,\n",
        "    #             \"f1_score\": float(f1)\n",
        "    #         }\n",
        "    #     ]\n",
        "\n",
        "    #     # Initialize BigQuery client\n",
        "    #     bqclient = bigquery.Client(project=project_id)\n",
        "\n",
        "    #     # Define the BigQuery table name\n",
        "    #     metadata_table = f\"{project_id}.MLOps.metrics\"\n",
        "\n",
        "    #     # Insert metrics into BigQuery table\n",
        "    #     errors = bqclient.insert_rows_json(metadata_table, rows_to_insert)\n",
        "\n",
        "    #     if errors:\n",
        "    #         print(f\"Error inserting data: {errors}\")\n",
        "    #     else:\n",
        "    #         print(f\"Data inserted successfully into {metadata_table}\")\n",
        "\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error during BigQuery insertion: {str(e)}\")\n",
        "    #     raise e  # Re-raise to propagate the failure\n",
        "\n",
        "    return f1\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------\n",
        "# Select Best Model\n",
        "@component(\n",
        "    base_image=\"python:3.10-slim\",\n",
        "    packages_to_install=[\"pandas==1.5.3\", \"numpy==1.23.5\"]\n",
        ")\n",
        "def select_best_model(\n",
        "    xgboost_f1: float,\n",
        "    naive_bayes_f1: float,\n",
        "    xgboost_model: Input[Model],\n",
        "    naive_bayes_model: Input[Model],\n",
        "    best_model: Output[Model]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Compare the F1 scores of XGBoost and Naive Bayes models and select the best model.\n",
        "    \"\"\"\n",
        "    import shutil\n",
        "\n",
        "    # Log the F1 scores for debugging\n",
        "    print(f\"XGBoost F1 Score: {xgboost_f1}\")\n",
        "    print(f\"Naive Bayes F1 Score: {naive_bayes_f1}\")\n",
        "\n",
        "    # Select the best model\n",
        "    if xgboost_f1 >= naive_bayes_f1:\n",
        "        print(\"XGBoost model is selected as the best model.\")\n",
        "        shutil.copytree(xgboost_model.path, best_model.path)\n",
        "        selected_model = \"XGBoost\"\n",
        "    else:\n",
        "        print(\"Naive Bayes model is selected as the best model.\")\n",
        "        shutil.copytree(naive_bayes_model.path, best_model.path)\n",
        "        selected_model = \"Naive Bayes\"\n",
        "\n",
        "    # Return the name of the selected model for tracking purposes\n",
        "    return selected_model\n"
      ],
      "metadata": {
        "id": "DEuK3O9qKr8s"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@pipeline(\n",
        "    name=\"to_be_final_v2\",\n",
        "    description=\"Model data pipeline - Training | Testing | Model Selection | Registration | Deployment\",\n",
        "    pipeline_root=_pipeline_artifacts_dir,\n",
        ")\n",
        "def model_data_pipeline(\n",
        "    start_year: int = 2018,\n",
        "    end_year: int = 2020,\n",
        "    limit: int = 100,\n",
        "    feature_name: str = 'complaint_english',\n",
        "    label_name: str = 'product',\n",
        "    model_display_name: str = \"best-complaints-model\",\n",
        "    endpoint_display_name: str = \"best-complaints-endpoint\",\n",
        "    deployed_model_display_name: str = \"best-complaints-deployment\"\n",
        "):\n",
        "    # Fetch data\n",
        "    get_data_component_task = get_data_component(\n",
        "        project_id=PROJECT_ID,\n",
        "        location=LOCATION,\n",
        "        start_year=start_year,\n",
        "        end_year=end_year,\n",
        "        feature_name=feature_name,\n",
        "        label_name=label_name,\n",
        "        testset_size=0.2,\n",
        "        limit=limit\n",
        "    )\n",
        "\n",
        "    # Train models\n",
        "    train_xgboost_task = train_xgboost_model(\n",
        "        train_data=get_data_component_task.outputs['train_data'],\n",
        "        feature_name=feature_name,\n",
        "        label_name=label_name\n",
        "    )\n",
        "\n",
        "    train_naive_bayes_task = train_naive_bayes_model(\n",
        "        train_data=get_data_component_task.outputs['train_data'],\n",
        "        feature_name=feature_name,\n",
        "        label_name=label_name\n",
        "    )\n",
        "\n",
        "    # Test models\n",
        "    test_xgboost_task = test_xgboost_model(\n",
        "        val_data=get_data_component_task.outputs['val_data'],\n",
        "        model_input=train_xgboost_task.outputs[\"model\"],\n",
        "        vectorizer_input=train_xgboost_task.outputs[\"vectorizer_output\"],\n",
        "        label_encoder_input=train_xgboost_task.outputs[\"label_encoder_output\"],\n",
        "        feature_name=feature_name,\n",
        "        label_name=label_name\n",
        "    )\n",
        "\n",
        "    test_naive_bayes_task = test_naive_bayes_model(\n",
        "        val_data=get_data_component_task.outputs['val_data'],\n",
        "        model_input=train_naive_bayes_task.outputs[\"model\"],\n",
        "        vectorizer_input=train_naive_bayes_task.outputs[\"vectorizer_output\"],\n",
        "        label_encoder_input=train_naive_bayes_task.outputs[\"label_encoder_output\"],\n",
        "        feature_name=feature_name,\n",
        "        label_name=label_name\n",
        "    )\n",
        "\n",
        "    # Select the best model based on F1 score\n",
        "    select_best_model_task = select_best_model(\n",
        "        xgboost_f1=test_xgboost_task.output,\n",
        "        naive_bayes_f1=test_naive_bayes_task.output,\n",
        "        xgboost_model=train_xgboost_task.outputs[\"model\"],\n",
        "        naive_bayes_model=train_naive_bayes_task.outputs[\"model\"]\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Detect Bias\n",
        "    bias_detection_task = bias_detection(\n",
        "        train_data=get_data_component_task.outputs['train_data'],\n",
        "        model_input=select_best_model_task.outputs[\"best_model\"],\n",
        "        vectorizer_input=train_xgboost_task.outputs[\"vectorizer_output\"],\n",
        "        label_encoder_input=train_xgboost_task.outputs[\"label_encoder_output\"],\n",
        "        feature_name=feature_name,\n",
        "        label_name=label_name\n",
        "    )\n",
        "\n",
        "    bias_detection_task.after(select_best_model_task)\n",
        "\n",
        "\n",
        "    # Register the selected model\n",
        "    model_registration_task = model_registration(\n",
        "        model_output=select_best_model_task.outputs[\"best_model\"],\n",
        "        project_id=PROJECT_ID,\n",
        "        location=LOCATION,\n",
        "        model_display_name=model_display_name\n",
        "    )\n",
        "\n",
        "    model_registration_task.after(bias_detection_task)\n",
        "    # Deploy the registered model\n",
        "    model_deployment_task = model_deployment(\n",
        "        model=model_registration_task.outputs[\"model\"],\n",
        "        project_id=PROJECT_ID,\n",
        "        location=LOCATION,\n",
        "        endpoint_display_name=endpoint_display_name,\n",
        "        deployed_model_display_name=deployed_model_display_name\n",
        "    )\n"
      ],
      "metadata": {
        "id": "8uXa2OXMLtmb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp import compiler\n",
        "from google.cloud import aiplatform\n",
        "from datetime import datetime\n",
        "\n",
        "# Compile the pipeline\n",
        "compiler.Compiler().compile(\n",
        "    pipeline_func=model_data_pipeline,\n",
        "    package_path=\"model_data_pipeline_job.json\"\n",
        ")\n",
        "\n",
        "# Generate a unique job ID using a timestamp\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "# Submit the job to Vertex AI\n",
        "job = aiplatform.PipelineJob(\n",
        "    display_name=\"model-data-pipeline\",\n",
        "    template_path=\"model_data_pipeline_job.json\",\n",
        "    job_id=f\"model-data-pipeline-{TIMESTAMP}\",\n",
        "    enable_caching=True,\n",
        "    pipeline_root=_pipeline_artifacts_dir,  # Make sure this path is set correctly\n",
        "    parameter_values={\n",
        "        \"start_year\": 2017,\n",
        "        \"end_year\": 2020,\n",
        "        \"limit\": 200,\n",
        "        \"feature_name\": \"complaint_english\",\n",
        "        \"label_name\": \"product\"\n",
        "    }\n",
        ")\n",
        "job.submit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVhfKSP62OKs",
        "outputId": "90e7abec-87d0-42b4-b1ae-76cfdf40863d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/661860051070/locations/us-east1/pipelineJobs/model-data-pipeline-20241116020659\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/661860051070/locations/us-east1/pipelineJobs/model-data-pipeline-20241116020659')\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/us-east1/pipelines/runs/model-data-pipeline-20241116020659?project=661860051070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jh9n0GvBHqV4"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}