{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "3NChGROd4vME",
        "outputId": "f4b81584-83dc-425f-af51-4848e7c10b35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (75.1.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.1.0\n",
            "    Uninstalling setuptools-75.1.0:\n",
            "      Successfully uninstalled setuptools-75.1.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-24.3.1 setuptools-75.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "setuptools"
                ]
              },
              "id": "6da71a9d819d45cba477b1cb77888e25"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jedi>=0.16"
      ],
      "metadata": {
        "id": "20_p4Czr4xdn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVp-ObuOxTWt",
        "outputId": "469f10cb-0471-4ff7-8f5e-2c915a343e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kfp\n",
            "  Using cached kfp-2.10.1-py3-none-any.whl\n",
            "Requirement already satisfied: click<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (8.1.7)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.16)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.8.0)\n",
            "Requirement already satisfied: kfp-pipeline-spec==0.5.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.5.0)\n",
            "Requirement already satisfied: kfp-server-api<2.4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.3.0)\n",
            "Requirement already satisfied: kubernetes<31,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (30.1.0)\n",
            "Requirement already satisfied: protobuf<5,>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (4.25.5)\n",
            "Requirement already satisfied: PyYAML<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.10.1)\n",
            "Requirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.9.0)\n",
            "Requirement already satisfied: urllib3<2.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (1.26.20)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.25.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.7.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp) (2.8.2)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<31,>=8.0.0->kfp) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<31,>=8.0.0->kfp) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes<31,>=8.0.0->kfp) (3.2.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3,>=2.2.1->kfp) (1.6.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.10)\n",
            "Installing collected packages: kfp\n",
            "Successfully installed kfp-2.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install kfp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "9kHnizd_xjdN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import kfp\n",
        "import json\n",
        "from datetime import datetime\n",
        "from kfp import compiler, dsl\n",
        "from typing import NamedTuple, List, Union, Dict, Any\n",
        "from google.cloud import aiplatform\n",
        "from kfp.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics, Dataset\n",
        "import requests\n",
        "\n",
        "print(f'KFB version: {kfp.__version__}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQyaTQ_AxkYB",
        "outputId": "57d98523-226c-4de4-ccac-713b892f4f2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KFB version: 2.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = 'bilingualcomplaint-system'\n",
        "LOCATION = 'us-east1'\n",
        "# Bucket Name\n",
        "GCS_artifacts_bucket_name = 'tfx-artifacts'\n",
        "# Pipeline\n",
        "pipeline_name = 'complaints-clf-vertex-training'\n",
        "# Path to various pipeline artifact.\n",
        "_pipeline_artifacts_dir = f'gs://{GCS_artifacts_bucket_name}/pipeline_artifacts/{pipeline_name}'\n",
        "\n",
        "aiplatform.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        "    staging_bucket=f'gs://{GCS_artifacts_bucket_name}',\n",
        "    )"
      ],
      "metadata": {
        "id": "4pDF0hxQx8An"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@component(\n",
        "    base_image=\"python:3.10-slim\",\n",
        "    packages_to_install = [\n",
        "        'google-cloud-bigquery==3.26.0',\n",
        "        'pandas==1.5.3',\n",
        "        'numpy==1.26.4',\n",
        "        'db-dtypes==1.3.0',\n",
        "        'scikit-learn==1.5.2'\n",
        "        ]\n",
        "    )\n",
        "def get_data_component(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    start_year: int, end_year: int,\n",
        "    feature_name: str,\n",
        "    label_name: str,\n",
        "    train_data: Output[Dataset],\n",
        "    val_data: Output[Dataset],\n",
        "    testset_size: float = 0.2,\n",
        "    limit:int=100):\n",
        "\n",
        "  from google.cloud import bigquery\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  import requests\n",
        "  from datetime import datetime\n",
        "  import smtplib\n",
        "  from email.mime.text import MIMEText\n",
        "  from email.mime.multipart import MIMEMultipart\n",
        "\n",
        "  # Track the start time of the component execution\n",
        "  start_time = datetime.now()\n",
        "\n",
        "  # Function to send custom Slack message with Kubeflow component details\n",
        "  def send_slack_message(component_name, execution_date, execution_time, duration):\n",
        "      SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'  # Replace with your Slack webhook URL\n",
        "      message = {\n",
        "          \"attachments\": [\n",
        "              {\n",
        "                  \"color\": \"#36a64f\",  # Green color for success\n",
        "                  \"pretext\": \":large_green_circle: Kubeflow Component Success Alert\",\n",
        "                  \"fields\": [\n",
        "                      {\n",
        "                          \"title\": \"Component Name\",\n",
        "                          \"value\": component_name,\n",
        "                          \"short\": True\n",
        "                      },\n",
        "                      {\n",
        "                          \"title\": \"Execution Date\",\n",
        "                          \"value\": execution_date,\n",
        "                          \"short\": True\n",
        "                      },\n",
        "                      {\n",
        "                          \"title\": \"Execution Time\",\n",
        "                          \"value\": execution_time,\n",
        "                          \"short\": True\n",
        "                      },\n",
        "                      {\n",
        "                          \"title\": \"Duration\",\n",
        "                          \"value\": f\"{duration} minutes\",\n",
        "                          \"short\": True\n",
        "                      }\n",
        "                  ]\n",
        "              }\n",
        "          ]\n",
        "      }\n",
        "\n",
        "      try:\n",
        "          response = requests.post(SLACK_WEBHOOK_URL, json=message)\n",
        "          response.raise_for_status()  # Check for request errors\n",
        "          pass\n",
        "      except requests.exceptions.RequestException as e:\n",
        "          pass\n",
        "\n",
        "  # Function to send success email\n",
        "  def send_success_email():\n",
        "      sender_email = \"sucessemailtrigger@gmail.com\"\n",
        "      password = \"jomnpxbfunwjgitb\"\n",
        "      receiver_emails = [\"hegde.anir@northeastern.edu\",\n",
        "                         \"nenavath.r@northeastern.edu\",\n",
        "                         \"pandey.raj@northeastern.edu\",\n",
        "                         \"khatri.say@northeastern.edu\",\n",
        "                         \"singh.arc@northeastern.edu\",\n",
        "                         \"goparaju.v@northeastern.edu\"]\n",
        "\n",
        "      # Current time for logging purposes\n",
        "      current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "      # Create the email content\n",
        "      subject = '[Kubeflow Pipeline] - Started'\n",
        "      body = f'''Hi team,\n",
        "\n",
        "      Model training in the Kubeflow pipeline has started!\n",
        "\n",
        "      Details:\n",
        "      - Start Time: {current_time}\n",
        "      - Dataset: {start_year}-{end_year}\n",
        "\n",
        "      Please monitor the pipeline for further updates.\n",
        "      '''\n",
        "\n",
        "      try:\n",
        "          # Set up the SMTP server\n",
        "          server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "          server.starttls()  # Secure the connection\n",
        "          server.login(sender_email, password)\n",
        "\n",
        "          # Send email to each receiver\n",
        "          for receiver_email in receiver_emails:\n",
        "              # Create a fresh message for each recipient\n",
        "              message = MIMEMultipart()\n",
        "              message['From'] = sender_email\n",
        "              message['To'] = receiver_email\n",
        "              message['Subject'] = subject\n",
        "              message.attach(MIMEText(body, 'plain'))\n",
        "\n",
        "              # Send the email\n",
        "              server.sendmail(sender_email, receiver_email, message.as_string())\n",
        "\n",
        "      except Exception as e:\n",
        "          pass\n",
        "      finally:\n",
        "          server.quit()\n",
        "\n",
        "  # Function to send failure email\n",
        "  def send_failure_email(error_message):\n",
        "      sender_email = \"sucessemailtrigger@gmail.com\"\n",
        "      password = \"jomnpxbfunwjgitb\"\n",
        "      receiver_emails = [\"hegde.anir@northeastern.edu\",\n",
        "                         \"nenavath.r@northeastern.edu\",\n",
        "                         \"pandey.raj@northeastern.edu\",\n",
        "                         \"khatri.say@northeastern.edu\",\n",
        "                         \"singh.arc@northeastern.edu\",\n",
        "                         \"goparaju.v@northeastern.edu\"]\n",
        "\n",
        "      # Create the email content\n",
        "      subject = '[Kubeflow Pipeline]'\n",
        "      body = f'Hi team,\\nModel training has failed!.\\nError Details: {error_message}'\n",
        "\n",
        "      try:\n",
        "          # Set up the SMTP server\n",
        "          server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "          server.starttls()  # Secure the connection\n",
        "          server.login(sender_email, password)\n",
        "\n",
        "          # Send email to each receiver\n",
        "          for receiver_email in receiver_emails:\n",
        "              # Create a fresh message for each recipient\n",
        "              message = MIMEMultipart()\n",
        "              message['From'] = sender_email\n",
        "              message['To'] = receiver_email\n",
        "              message['Subject'] = subject\n",
        "              message.attach(MIMEText(body, 'plain'))\n",
        "\n",
        "              # Send the email\n",
        "              server.sendmail(sender_email, receiver_email, message.as_string())\n",
        "\n",
        "      except Exception as e:\n",
        "          pass\n",
        "      finally:\n",
        "          server.quit()\n",
        "\n",
        "  try:\n",
        "    bqclient = bigquery.Client(project=project_id, location=location)\n",
        "\n",
        "    QUERY = f'''select complaint_english, complaint_hindi, product from `bilingualcomplaint-system.MLOps`.get_dataset_by_complaint_year_interval({start_year}, {end_year}) limit {limit}'''\n",
        "    query_job = bqclient.query(QUERY)  # API request\n",
        "    rows = query_job.result()  # Waits for query to finish\n",
        "    data = rows.to_dataframe()\n",
        "\n",
        "    products = data['product'].tolist()\n",
        "    # Encode product labels\n",
        "    product_encoder = LabelEncoder()\n",
        "    encoded_products  = product_encoder.fit_transform(products)\n",
        "    num_labels = len(product_encoder.classes_)\n",
        "    # Melt the DataFrame\n",
        "    data_features = data.melt(\n",
        "        id_vars=[\"product\"],  # Columns to keep as-is\n",
        "        value_vars=[\"complaint_english\", \"complaint_hindi\"],  # Columns to melt\n",
        "        value_name=\"complaints\"  # New column name for melted values\n",
        "    )\n",
        "\n",
        "    train, val = train_test_split(data_features[[feature_name, label_name]], test_size=testset_size, random_state=42)\n",
        "    train.reset_index(drop=True, inplace=True)\n",
        "    val.reset_index(drop=True, inplace=True)\n",
        "    train.to_pickle(train_data.path)\n",
        "    val.to_pickle(val_data.path)\n",
        "    # Track the end time and calculate duration\n",
        "    end_time = datetime.now()\n",
        "    duration = (end_time - start_time).total_seconds() / 60  # Duration in minutes\n",
        "\n",
        "    # Send success email once the data is processed\n",
        "    send_success_email()\n",
        "    # Send the Slack message with execution details\n",
        "    send_slack_message(\n",
        "        component_name=\"Getting Data Component\",\n",
        "        execution_date=end_time.strftime('%Y-%m-%d'),\n",
        "        execution_time=end_time.strftime('%H:%M:%S'),\n",
        "        duration=round(duration, 2)  # Round duration to 2 decimal places\n",
        "    )\n",
        "\n",
        "  except Exception as e:\n",
        "      # Send failure email if there's an error\n",
        "      error_message = str(e)\n",
        "      send_failure_email(error_message)\n",
        "      send_slack_message(\n",
        "          component_name=\"Model Training Component Failed\",\n",
        "          execution_date=datetime.now().strftime('%Y-%m-%d'),\n",
        "          execution_time=datetime.now().strftime('%H:%M:%S'),\n",
        "          duration=0  # If failed, duration is 0\n",
        "      )"
      ],
      "metadata": {
        "id": "a6gfsDfZx_jT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@component(\n",
        "    base_image=\"tensorflow/tensorflow:2.14.0\",\n",
        "    packages_to_install = [\n",
        "        'pandas==1.5.3',\n",
        "        'numpy==1.26.4',\n",
        "        'transformers==4.44.2'\n",
        "        ]\n",
        "    )\n",
        "def prepare_data_component(\n",
        "    data: Input[Dataset],\n",
        "    tf_dataset: Output[Dataset],\n",
        "    dataset_name: str,\n",
        "    feature_name: str, label_name: str,\n",
        "    label_map: Dict[str, int],\n",
        "    batch_size:int = 32,\n",
        "    max_sequence_length: int = 128,\n",
        "    hugging_face_model_name: str = 'bert-base-multilingual-cased'\n",
        "    ):\n",
        "  import os\n",
        "  import pickle\n",
        "  import pandas as pd\n",
        "  import tensorflow as tf\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  # Function to serialize each example\n",
        "  def serialize_example(feature, label):\n",
        "    feature = tf.train.Feature(int64_list=tf.train.Int64List(value=feature))\n",
        "    label = tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
        "    feature_dict = {\n",
        "        'feature': feature,\n",
        "        'label': label\n",
        "    }\n",
        "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
        "    return example_proto.SerializeToString()\n",
        "\n",
        "  tokenizer = BertTokenizer.from_pretrained(hugging_face_model_name)\n",
        "\n",
        "  # Load the pickled dataset\n",
        "  with open(data.path, 'rb') as f:\n",
        "    loaded_dataframe: pd.DataFrame  = pickle.load(f)\n",
        "\n",
        "  # Create a TextVectorization layer\n",
        "  max_features = 1000  # Adjust based on your vocabulary size\n",
        "  sequence_length = 450  # Adjust based on your text length\n",
        "\n",
        "  vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "      standardize=\"lower_and_strip_punctuation\",\n",
        "      max_tokens=max_features,\n",
        "      output_mode='int',\n",
        "      output_sequence_length=sequence_length)\n",
        "\n",
        "  # Adapt the layer to your text data\n",
        "  vectorize_layer.adapt(loaded_dataframe[feature_name])\n",
        "\n",
        "\n",
        "  # Create a dictionary to map each unique label in 'product' to an integer\n",
        "  labels = loaded_dataframe[label_name].map(label_map).values\n",
        "\n",
        "  # Convert labels to tensor format for compatibility with TensorFlow\n",
        "  labels_tf = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
        "\n",
        "  features_tf = tokenizer(\n",
        "      loaded_dataframe[feature_name].tolist(),\n",
        "      padding=True, truncation=True,\n",
        "      return_tensors=\"tf\",\n",
        "      max_length=max_sequence_length)[\"input_ids\"]\n",
        "\n",
        "  # Create TensorFlow datasets\n",
        "  tf_prepared_dataset = tf.data.Dataset.from_tensor_slices((features_tf, labels_tf))\n",
        "\n",
        "  # make the folder if does not exist\n",
        "  os.makedirs(tf_dataset.path, exist_ok=True)\n",
        "\n",
        "  tfrecord_file_path = os.path.join(tf_dataset.path, f'{dataset_name}.tfrecord')\n",
        "\n",
        "  # Write the dataset to TFRecord\n",
        "  with tf.io.TFRecordWriter(tfrecord_file_path) as writer:\n",
        "    for feature, label in tf_prepared_dataset:\n",
        "      serialized_example = serialize_example(feature, label)\n",
        "      writer.write(serialized_example)"
      ],
      "metadata": {
        "id": "tCB84HUQ0v0r"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@component(\n",
        "    base_image=\"tensorflow/tensorflow:2.14.0\",\n",
        "    packages_to_install = [\n",
        "        'pandas==1.5.3',\n",
        "        'numpy==1.26.4',\n",
        "        'transformers==4.44.2',\n",
        "        'keras==2.14.0'\n",
        "        ]\n",
        "    )\n",
        "def train_mbert_model(\n",
        "    train_data: Input[Dataset],\n",
        "    model_output: Output[Model],\n",
        "    train_data_name: str,\n",
        "    model_params: Dict[str, Any],\n",
        "    label_map: Dict[str, int],\n",
        "    model_save_name: str = 'saved_mbert_model',\n",
        "    huggingface_model_name: str = 'bert-base-multilingual-cased',\n",
        "    max_epochs: int = 2,\n",
        "    train_size: float = 0.8,\n",
        "    batch_size: int = 8,\n",
        "    max_sequence_length: int = 128,\n",
        "    ):\n",
        "  import os\n",
        "  import tensorflow as tf\n",
        "  from transformers import BertTokenizer\n",
        "  from transformers import TFBertForSequenceClassification, BertTokenizer\n",
        "  from datetime import datetime\n",
        "  import requests\n",
        "\n",
        "  # Track the start time of the component execution\n",
        "  start_time = datetime.now()\n",
        "\n",
        "  # Function to send custom Slack message with Kubeflow component details\n",
        "  def send_slack_message(component_name, execution_date, execution_time, duration):\n",
        "      SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'  # Replace with your Slack webhook URL\n",
        "      message = {\n",
        "          \"attachments\": [\n",
        "              {\n",
        "                  \"color\": \"#36a64f\",  # Green color for success\n",
        "                  \"pretext\": \":large_green_circle: Kubeflow Component Success Alert\",\n",
        "                  \"fields\": [\n",
        "                      {\n",
        "                          \"title\": \"Component Name\",\n",
        "                          \"value\": component_name,\n",
        "                          \"short\": True\n",
        "                      },\n",
        "                      {\n",
        "                          \"title\": \"Execution Date\",\n",
        "                          \"value\": execution_date,\n",
        "                          \"short\": True\n",
        "                      },\n",
        "                      {\n",
        "                          \"title\": \"Execution Time\",\n",
        "                          \"value\": execution_time,\n",
        "                          \"short\": True\n",
        "                      },\n",
        "                      {\n",
        "                          \"title\": \"Duration\",\n",
        "                          \"value\": f\"{duration} minutes\",\n",
        "                          \"short\": True\n",
        "                      }\n",
        "                  ]\n",
        "              }\n",
        "          ]\n",
        "      }\n",
        "\n",
        "      try:\n",
        "          response = requests.post(SLACK_WEBHOOK_URL, json=message)\n",
        "          response.raise_for_status()  # Check for request errors\n",
        "          pass\n",
        "      except requests.exceptions.RequestException as e:\n",
        "          pass\n",
        "\n",
        "  def parse_tfrecord_fn(example_proto, max_sequence_length=max_sequence_length):\n",
        "    # Define the feature description dictionary\n",
        "    feature_description = {\n",
        "        'feature': tf.io.FixedLenFeature([max_sequence_length], tf.int64),  # Adjust the shape [128] as per your data\n",
        "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "        }\n",
        "    # Parse the input tf.train.Example proto using the feature description\n",
        "    parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
        "    return parsed_example['feature'], parsed_example['label']\n",
        "\n",
        "  # Define BERT model\n",
        "  def build_bert_model(vocab_size, embedding_dim=128, lstm_units=64, num_classes=2):\n",
        "      model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_classes)\n",
        "\n",
        "      return model\n",
        "\n",
        "  try:\n",
        "    tokenizer = BertTokenizer.from_pretrained(huggingface_model_name)\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "    record_path = os.path.join(train_data.path, f'{train_data_name}.tfrecord')\n",
        "    train_dataset = tf.data.TFRecordDataset(record_path)\n",
        "    train_dataset = train_dataset.map(parse_tfrecord_fn)\n",
        "    train_dataset = train_dataset.cache()\n",
        "\n",
        "    # Determine the total number of elements in the train_dataset\n",
        "    total_size = len(list(train_dataset.as_numpy_iterator()))\n",
        "    train_size = int(train_size * total_size)\n",
        "    val_size = total_size - train_size\n",
        "\n",
        "    # Split the dataset\n",
        "    train_subset = train_dataset.take(train_size)\n",
        "    validation_subset = train_dataset.skip(train_size)\n",
        "\n",
        "    train_subset = train_subset.batch(batch_size).shuffle(buffer_size=train_size)\n",
        "    validation_subset = validation_subset.batch(batch_size)\n",
        "\n",
        "    model = build_bert_model(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=model_params.get('embed_dim', 128),\n",
        "        lstm_units=model_params.get('hidden_dim', 128),\n",
        "        num_classes=len(label_map)\n",
        "    )\n",
        "\n",
        "    optimizer = tf.optimizers.Adam(learning_rate=3e-5)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "    history = model.fit(\n",
        "          train_subset,\n",
        "          validation_data=validation_subset,\n",
        "          epochs=max_epochs\n",
        "          )\n",
        "\n",
        "    os.makedirs(model_output.path, exist_ok=True)\n",
        "\n",
        "    model.save(os.path.join(model_output.path, f'{model_save_name}.keras'))\n",
        "    model.save(os.path.join(model_output.path))\n",
        "\n",
        "    # Track the end time and calculate duration\n",
        "    end_time = datetime.now()\n",
        "    duration = (end_time - start_time).total_seconds() / 60  # Duration in minutes\n",
        "\n",
        "    # Send the Slack message with execution details\n",
        "    send_slack_message(\n",
        "        component_name=\"Model Training Component\",\n",
        "        execution_date=end_time.strftime('%Y-%m-%d'),\n",
        "        execution_time=end_time.strftime('%H:%M:%S'),\n",
        "        duration=round(duration, 2)  # Round duration to 2 decimal places\n",
        "    )\n",
        "\n",
        "  except Exception as e:\n",
        "      error_message = str(e)\n",
        "      send_slack_message(\n",
        "          component_name=\"Model Training Component\",\n",
        "          execution_date=datetime.now().strftime('%Y-%m-%d'),\n",
        "          execution_time=datetime.now().strftime('%H:%M:%S'),\n",
        "          duration=0  # If failed, duration is 0\n",
        "      )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uJ0EKlbRbCeG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST M-BERT component\n",
        "@component(\n",
        "    base_image=\"tensorflow/tensorflow:2.14.0\",\n",
        "    packages_to_install = [\n",
        "        'pandas==1.5.3',\n",
        "        'numpy==1.26.4',\n",
        "        'transformers==4.44.2',\n",
        "        'scikit-learn==1.5.2',\n",
        "        'keras==2.14.0'\n",
        "    ]\n",
        ")\n",
        "def test_mbert_model(\n",
        "    test_data: Input[Dataset],\n",
        "    trained_model: Input[Model],\n",
        "    metric: Output[Metrics],\n",
        "    reusable_model: Output[Model],  # Reference trained model path\n",
        "    metrics_dict: Output[Artifact],\n",
        "    test_data_name: str,\n",
        "    label_map: Dict[str, int],\n",
        "    model_name: str = 'saved_mbert_model',\n",
        "    batch_size: int = 8,\n",
        "    max_sequence_length: int = 128,\n",
        "    huggingface_model_name: str = 'bert-base-multilingual-cased'\n",
        "):\n",
        "    import os\n",
        "    import tensorflow as tf\n",
        "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "    from transformers import TFBertForSequenceClassification\n",
        "    import requests\n",
        "    from datetime import datetime\n",
        "    import json\n",
        "\n",
        "    # Track the start time of the component execution\n",
        "    start_time = datetime.now()\n",
        "    # Function to send custom Slack message with Kubeflow component details\n",
        "    def send_slack_message(component_name, execution_date, execution_time, duration, f1_score=None, precision=None, recall=None):\n",
        "        SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'\n",
        "        message = {\n",
        "            \"attachments\": [\n",
        "                {\n",
        "                    \"color\": \"#36a64f\",  # Green color for success\n",
        "                    \"pretext\": \":large_green_circle: Kubeflow Component Success Alert\",\n",
        "                    \"fields\": [\n",
        "                        {\n",
        "                            \"title\": \"Component Name\",\n",
        "                            \"value\": component_name,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Execution Date\",\n",
        "                            \"value\": execution_date,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Execution Time\",\n",
        "                            \"value\": execution_time,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Duration\",\n",
        "                            \"value\": f\"{duration} minutes\",\n",
        "                            \"short\": True\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        if f1_score is not None:\n",
        "            message[\"attachments\"][0][\"fields\"].append({\n",
        "                \"title\": \"Validation F1 Score\",\n",
        "                \"value\": f\"{f1_score:.4f}\",\n",
        "                \"short\": True\n",
        "            })\n",
        "\n",
        "        if precision is not None:\n",
        "            message[\"attachments\"][0][\"fields\"].append({\n",
        "                \"title\": \"Validation Precision\",\n",
        "                \"value\": f\"{precision:.4f}\",\n",
        "                \"short\": True\n",
        "            })\n",
        "\n",
        "        if recall is not None:\n",
        "            message[\"attachments\"][0][\"fields\"].append({\n",
        "                \"title\": \"Validation Recall\",\n",
        "                \"value\": f\"{recall:.4f}\",\n",
        "                \"short\": True\n",
        "            })\n",
        "\n",
        "        try:\n",
        "            response = requests.post(SLACK_WEBHOOK_URL, json=message)\n",
        "            response.raise_for_status()  # Check for request errors\n",
        "        except requests.exceptions.RequestException as e:\n",
        "          pass\n",
        "\n",
        "    def parse_tfrecord_fn(example_proto, max_sequence_length=max_sequence_length):\n",
        "      feature_description = {\n",
        "          'feature': tf.io.FixedLenFeature([max_sequence_length], tf.int64),\n",
        "          'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "      }\n",
        "      parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
        "      return parsed_example['feature'], parsed_example['label']\n",
        "\n",
        "    try:\n",
        "      # Load test dataset\n",
        "      record_path = os.path.join(test_data.path, f'{test_data_name}.tfrecord')\n",
        "      test_dataset = tf.data.TFRecordDataset(record_path)\n",
        "      test_dataset = test_dataset.map(parse_tfrecord_fn).batch(batch_size)\n",
        "\n",
        "      # Load model from the trained model path\n",
        "      model_path = os.path.join(trained_model.path, f'{model_name}.keras')\n",
        "      model = TFBertForSequenceClassification.from_pretrained(\n",
        "          huggingface_model_name,\n",
        "          num_labels=len(label_map)\n",
        "      )\n",
        "      model.load_weights(model_path)\n",
        "\n",
        "      ########model = tf.keras.models.load_model(trained_model.path)  # Load the model in SavedModel format\n",
        "\n",
        "      # Get predictions\n",
        "      predictions = model.predict(test_dataset)\n",
        "      logits = predictions.logits\n",
        "\n",
        "      # Calculate metrics\n",
        "      y_true = [label for _, label in list(test_dataset.unbatch().as_numpy_iterator())]\n",
        "      y_pred = list(tf.argmax(logits, axis=1).numpy())\n",
        "\n",
        "      precision = precision_score(y_true, y_pred, average='weighted')\n",
        "      recall = recall_score(y_true, y_pred, average='weighted')\n",
        "      f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "      metric.log_metric('precision', precision)\n",
        "      metric.log_metric('recall', recall)\n",
        "      metric.log_metric('f1', f1)\n",
        "      metrics = {'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "      # Reference the trained model path\n",
        "      reusable_model.uri = trained_model.uri  # Reuse the same path\n",
        "\n",
        "      print(f'Precision: {precision:.4f}')\n",
        "      print(f'Recall: {recall:.4f}')\n",
        "      print(f'F1 Score: {f1:.4f}')\n",
        "      print(f'Referenced trained model path: {reusable_model.uri}')\n",
        "\n",
        "      # Ensure the directory exists\n",
        "      os.makedirs(os.path.dirname(metrics_dict.path), exist_ok=True)\n",
        "\n",
        "      # Save metrics to an output artifact\n",
        "      metrics = {'precision': precision, 'recall': recall, 'f1': f1}\n",
        "      with open(metrics_dict.path, 'w') as f:\n",
        "          json.dump(metrics, f)\n",
        "\n",
        "      # Track the end time and calculate duration\n",
        "      end_time = datetime.now()\n",
        "      duration = (end_time - start_time).total_seconds() / 60  # Duration in minutes\n",
        "\n",
        "      # Send the Slack message with execution details and metrics\n",
        "      send_slack_message(\n",
        "          component_name=\"Model Testing Component\",\n",
        "          execution_date=end_time.strftime('%Y-%m-%d'),\n",
        "          execution_time=end_time.strftime('%H:%M:%S'),\n",
        "          duration=round(duration, 2),\n",
        "          f1_score=f1,\n",
        "          precision=precision,\n",
        "          recall=recall\n",
        "      )\n",
        "\n",
        "    except Exception as e:\n",
        "      error_message = str(e)\n",
        "      print(f\"Error during model testing: {error_message}\")\n",
        "      send_slack_message(\n",
        "          component_name=\"Model Testing Component Failed\",\n",
        "          execution_date=datetime.now().strftime('%Y-%m-%d'),\n",
        "          execution_time=datetime.now().strftime('%H:%M:%S'),\n",
        "          duration= 0  # If failed, duration is 0\n",
        "      )\n",
        "      raise e\n",
        "\n"
      ],
      "metadata": {
        "id": "KaCgDfnzbEZf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@component(\n",
        "    base_image=\"python:3.10-slim\",\n",
        "    packages_to_install=[\n",
        "        'pandas==1.5.3',\n",
        "        'numpy==1.26.4',\n",
        "        'tensorflow==2.14.0',\n",
        "        'fairlearn==0.8.0',\n",
        "        'scikit-learn==1.5.2',\n",
        "        'transformers==4.44.2',\n",
        "        'json5'\n",
        "    ]\n",
        ")\n",
        "def bias_detection(\n",
        "    test_data: Input[Dataset],\n",
        "    trained_model: Input[Model],\n",
        "    bias_report: Output[Dataset],\n",
        "    feature_name: str,\n",
        "    label_name: str,\n",
        "    test_data_name: str,\n",
        "    label_map: Dict[str, int],\n",
        "    model_name: str = 'saved_mbert_model',\n",
        "    batch_size: int = 8,\n",
        "    max_sequence_length: int = 128,\n",
        "    huggingface_model_name: str = 'bert-base-multilingual-cased',\n",
        "    accuracy_threshold: float = 0.2\n",
        "):\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    import tensorflow as tf\n",
        "    from fairlearn.metrics import MetricFrame, true_positive_rate, false_positive_rate, selection_rate\n",
        "    from transformers import TFBertForSequenceClassification\n",
        "    import requests\n",
        "    import json\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Hardcoded label map\n",
        "    idx_2_label_map = {\n",
        "        0: 'Credit reporting, credit repair services, or other personal consumer reports',\n",
        "        1: 'Debt collection',\n",
        "        2: 'Checking or savings account',\n",
        "        3: 'Credit card or prepaid card',\n",
        "        4: 'Mortgage',\n",
        "        5: 'Money transfer, virtual currency, or money service',\n",
        "        6: 'Vehicle loan or lease',\n",
        "        7: 'Student loan'\n",
        "    }\n",
        "\n",
        "    # Function to send alert\n",
        "    def send_alert(summary_message):\n",
        "        SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'\n",
        "        payload = {\"text\": summary_message}\n",
        "        try:\n",
        "            response = requests.post(SLACK_WEBHOOK_URL, json=payload)\n",
        "            response.raise_for_status()\n",
        "            print(f\"Alert sent successfully: {summary_message}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to send alert: {str(e)}\")\n",
        "\n",
        "    print(f\"Starting bias detection component...\")\n",
        "    print(f\"Test data path: {test_data.path}\")\n",
        "    print(f\"Trained model path: {trained_model.path}\")\n",
        "\n",
        "          # Load and parse test dataset\n",
        "    def parse_tfrecord_fn(example_proto, max_sequence_length=max_sequence_length):\n",
        "      feature_description = {\n",
        "          'feature': tf.io.FixedLenFeature([max_sequence_length], tf.int64),\n",
        "          'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "      }\n",
        "      parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
        "      return parsed_example['feature'], parsed_example['label']\n",
        "\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading test dataset...\")\n",
        "        record_path = os.path.join(test_data.path, f'{test_data_name}.tfrecord')\n",
        "        print(f\"Record path: {record_path}\")\n",
        "        test_dataset = tf.data.TFRecordDataset(record_path)\n",
        "        test_dataset = test_dataset.map(parse_tfrecord_fn).batch(batch_size)\n",
        "\n",
        "        print(f\"Loading trained model...\")\n",
        "        model_path = os.path.join(trained_model.path, f'{model_name}.keras')\n",
        "        print(f\"Model path: {model_path}\")\n",
        "        model = TFBertForSequenceClassification.from_pretrained(\n",
        "            huggingface_model_name,\n",
        "            num_labels=len(label_map)\n",
        "        )\n",
        "        model.load_weights(model_path)\n",
        "\n",
        "        print(f\"Making predictions...\")\n",
        "        y_true, y_pred = [], []\n",
        "        for features, labels in test_dataset.unbatch().as_numpy_iterator():\n",
        "            logits = model(tf.expand_dims(features, axis=0)).logits\n",
        "            predicted_label = tf.argmax(logits, axis=1).numpy()[0]\n",
        "\n",
        "            y_true.append(labels)\n",
        "            y_pred.append(predicted_label)\n",
        "\n",
        "        print(f\"Building DataFrame for Fairlearn slicing...\")\n",
        "        df = pd.DataFrame({\n",
        "            'true_label': y_true,\n",
        "            'predicted_label': y_pred\n",
        "        })\n",
        "\n",
        "\n",
        "        # Define multiclass metrics\n",
        "        def multiclass_metric(metric_fn, y_true, y_pred, label):\n",
        "            \"\"\"Compute a metric for a specific label in a one-vs-rest approach.\"\"\"\n",
        "            y_true_binary = (y_true == label).astype(int)\n",
        "            y_pred_binary = (y_pred == label).astype(int)\n",
        "            return metric_fn(y_true_binary, y_pred_binary)\n",
        "\n",
        "        metrics = {\n",
        "            'accuracy': lambda y_true, y_pred: (y_true == y_pred).mean(),\n",
        "            'true_positive_rate': lambda y_true, y_pred: {\n",
        "                label: multiclass_metric(true_positive_rate, y_true, y_pred, label)\n",
        "                for label in set(y_true)\n",
        "            },\n",
        "            'false_positive_rate': lambda y_true, y_pred: {\n",
        "                label: multiclass_metric(false_positive_rate, y_true, y_pred, label)\n",
        "                for label in set(y_true)\n",
        "            },\n",
        "            'selection_rate': lambda y_true, y_pred: {\n",
        "                label: multiclass_metric(selection_rate, y_true, y_pred, label)\n",
        "                for label in set(y_true)\n",
        "            },\n",
        "        }\n",
        "\n",
        "        # Calculate metrics using Fairlearn\n",
        "        print(\"Calculating metrics using Fairlearn...\")\n",
        "        metric_frame = MetricFrame(\n",
        "            metrics=metrics,\n",
        "            y_true=df['true_label'],\n",
        "            y_pred=df['predicted_label'],\n",
        "            sensitive_features=df['true_label']\n",
        "        )\n",
        "        bias_report_df = metric_frame.by_group.reset_index()\n",
        "\n",
        "        # Identify slices with low accuracy\n",
        "        print(f\"Checking for accuracy below threshold ({accuracy_threshold})...\")\n",
        "        grouping_column = 'true_label' if 'true_label' in bias_report_df.columns else bias_report_df.columns[0]\n",
        "        low_accuracy_slices = bias_report_df[bias_report_df['accuracy'] < accuracy_threshold]\n",
        "\n",
        "        # Generate alert if necessary\n",
        "        if not low_accuracy_slices.empty:\n",
        "            low_accuracy_slices_str = \"\\n\".join([\n",
        "                f\"- {idx_2_label_map.get(row[grouping_column], 'Unknown')}: Accuracy {row['accuracy']:.2f}\"\n",
        "                for _, row in low_accuracy_slices.iterrows()\n",
        "            ])\n",
        "            summary_message = (\n",
        "                f\"Bias Alert: The following slices have accuracy below the threshold of {accuracy_threshold:.2f}:\\n\"\n",
        "                f\"{low_accuracy_slices_str}\\n\\n\"\n",
        "                f\"**Action Required:** Please mitigate this issue as soon as possible to ensure fairness in the model.\"\n",
        "            )\n",
        "            print(summary_message)\n",
        "            send_alert(summary_message)\n",
        "        else:\n",
        "            print(\"No bias detected. All slices meet the accuracy threshold.\")\n",
        "\n",
        "        print(\"Bias detection completed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in bias detection: {str(e)}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "copSvo7EGbTN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "        # metrics = {\n",
        "        #     'accuracy': lambda y_true, y_pred: (y_true == y_pred).mean(),\n",
        "        #     'true_positive_rate': lambda y_true, y_pred: {\n",
        "        #         label: true_positive_rate((y_true == label).astype(int), (y_pred == label).astype(int))\n",
        "        #         for label in set(y_true)\n",
        "        #     },\n",
        "        #     'false_positive_rate': lambda y_true, y_pred: {\n",
        "        #         label: false_positive_rate((y_true == label).astype(int), (y_pred == label).astype(int))\n",
        "        #         for label in set(y_true)\n",
        "        #     },\n",
        "        #     'selection_rate': lambda y_true, y_pred: {\n",
        "        #         label: selection_rate((y_true == label).astype(int), (y_pred == label).astype(int))\n",
        "        #         for label in set(y_true)\n",
        "        #     },\n",
        "        # }\n",
        "\n",
        "        # metric_frame = MetricFrame(metrics=metrics, y_true=df['true_label'], y_pred=df['predicted_label'], sensitive_features=df['true_label'])\n",
        "        # bias_report_df = metric_frame.by_group.reset_index()\n",
        "\n",
        "        # bias_report.uri = f\"gs://bias_reports/bias_report.csv\"\n",
        "\n"
      ],
      "metadata": {
        "id": "EbP9M9wbRd66"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@component(\n",
        "  packages_to_install=[\"google-cloud-aiplatform\", \"google-auth\"]\n",
        ")\n",
        "def model_registration(\n",
        "  model_output: Input[Model],\n",
        "  project_id: str,\n",
        "  location: str,\n",
        "  model_display_name: str,\n",
        "  model: Output[Model]\n",
        "):\n",
        "  from google.cloud import aiplatform\n",
        "\n",
        "  # Initialize Vertex AI SDK\n",
        "  aiplatform.init(project=project_id, location=location)\n",
        "\n",
        "  # Use the URI directly without appending \"/model\"\n",
        "  model_uri = model_output.uri\n",
        "\n",
        "  # Check if a model with the same display name already exists\n",
        "  existing_models = aiplatform.Model.list(\n",
        "      filter=f'display_name=\"{model_display_name}\"',\n",
        "      order_by=\"create_time desc\",\n",
        "      project=project_id,\n",
        "      location=location\n",
        "  )\n",
        "\n",
        "  if existing_models:\n",
        "      # Get the first existing model's resource name (ID) to add a new version under it\n",
        "      parent_model = existing_models[0]\n",
        "      model_id = parent_model.resource_name\n",
        "\n",
        "      # Register the model under the existing model ID\n",
        "      registered_model = aiplatform.Model.upload(\n",
        "          display_name=model_display_name,\n",
        "          artifact_uri=model_uri,\n",
        "          serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\",  # TensorFlow Serving Image\n",
        "          parent_model=model_id  # Use the existing model's ID to create a new version\n",
        "      )\n",
        "  else:\n",
        "      # No existing model, create a new one\n",
        "      registered_model = aiplatform.Model.upload(\n",
        "          display_name=model_display_name,\n",
        "          artifact_uri=model_uri,\n",
        "          serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\"  # TensorFlow Serving Image\n",
        "      )\n",
        "\n",
        "  # Output the model resource name\n",
        "  model.uri = registered_model.resource_name\n"
      ],
      "metadata": {
        "id": "VeT09ZxgvcCZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "021c7629-d2e8-472a-d33c-494e59063369"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/kfp/dsl/component_decorator.py:126: FutureWarning: The default base_image used by the @dsl.component decorator will switch from 'python:3.9' to 'python:3.10' on Oct 1, 2025. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.10.\n",
            "  return component_factory.create_component_from_func(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@component(\n",
        "    packages_to_install=[\"google-cloud-aiplatform\"]\n",
        ")\n",
        "def model_deployment(\n",
        "    model: Input[Model],  # This is a string (model URI)\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    endpoint_display_name: str,\n",
        "    deployed_model_display_name: str,\n",
        "    endpoint: Output[Artifact]\n",
        "):\n",
        "    from google.cloud import aiplatform, bigquery\n",
        "    import requests\n",
        "    from datetime import datetime\n",
        "    from email.mime.multipart import MIMEMultipart\n",
        "    from email.mime.text import MIMEText\n",
        "    import smtplib\n",
        "\n",
        "    # Function to send success email\n",
        "    def send_success_email():\n",
        "        sender_email = \"sucessemailtrigger@gmail.com\"\n",
        "        password = \"jomnpxbfunwjgitb\"\n",
        "        receiver_emails = [\"hegde.anir@northeastern.edu\",\n",
        "                          \"nenavath.r@northeastern.edu\",\n",
        "                          \"pandey.raj@northeastern.edu\",\n",
        "                          \"khatri.say@northeastern.edu\",\n",
        "                          \"singh.arc@northeastern.edu\",\n",
        "                          \"goparaju.v@northeastern.edu\"]\n",
        "\n",
        "\n",
        "        # Current time for logging purposes\n",
        "        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        # Create the email content\n",
        "        subject = '[Kubeflow Pipeline] - Completed'\n",
        "        body = f'''Hi team,\n",
        "\n",
        "        Model has been deployed!\n",
        "\n",
        "        Details:\n",
        "        - Start Time: {current_time}\n",
        "\n",
        "\n",
        "        '''\n",
        "\n",
        "        try:\n",
        "            # Set up the SMTP server\n",
        "            server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "            server.starttls()  # Secure the connection\n",
        "            server.login(sender_email, password)\n",
        "\n",
        "            # Send email to each receiver\n",
        "            for receiver_email in receiver_emails:\n",
        "                # Create a fresh message for each recipient\n",
        "                message = MIMEMultipart()\n",
        "                message['From'] = sender_email\n",
        "                message['To'] = receiver_email\n",
        "                message['Subject'] = subject\n",
        "                message.attach(MIMEText(body, 'plain'))\n",
        "\n",
        "                # Send the email\n",
        "                server.sendmail(sender_email, receiver_email, message.as_string())\n",
        "\n",
        "        except Exception as e:\n",
        "            pass\n",
        "        finally:\n",
        "            server.quit()\n",
        "\n",
        "    # Function to send failure email\n",
        "    def send_failure_email(error_message):\n",
        "        sender_email = \"sucessemailtrigger@gmail.com\"\n",
        "        password = \"jomnpxbfunwjgitb\"\n",
        "        receiver_emails = [\"hegde.anir@northeastern.edu\",\n",
        "                          \"nenavath.r@northeastern.edu\",\n",
        "                          \"pandey.raj@northeastern.edu\",\n",
        "                          \"khatri.say@northeastern.edu\",\n",
        "                          \"singh.arc@northeastern.edu\",\n",
        "                          \"goparaju.v@northeastern.edu\"]\n",
        "\n",
        "        # Create the email content\n",
        "        subject = '[Kubeflow Pipeline]'\n",
        "        body = f'Hi team,\\nModel deployment has failed!.\\nError Details: {error_message}'\n",
        "\n",
        "        try:\n",
        "            # Set up the SMTP server\n",
        "            server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "            server.starttls()  # Secure the connection\n",
        "            server.login(sender_email, password)\n",
        "\n",
        "            # Send email to each receiver\n",
        "            for receiver_email in receiver_emails:\n",
        "                # Create a fresh message for each recipient\n",
        "                message = MIMEMultipart()\n",
        "                message['From'] = sender_email\n",
        "                message['To'] = receiver_email\n",
        "                message['Subject'] = subject\n",
        "                message.attach(MIMEText(body, 'plain'))\n",
        "\n",
        "                # Send the email\n",
        "                server.sendmail(sender_email, receiver_email, message.as_string())\n",
        "\n",
        "        except Exception as e:\n",
        "            pass\n",
        "        finally:\n",
        "            server.quit()\n",
        "\n",
        "    # Function to send custom Slack message with Kubeflow component details\n",
        "    def send_slack_message(component_name, execution_date, execution_time, duration, endpoint_name):\n",
        "        SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'  # Replace with your Slack webhook URL\n",
        "        message = {\n",
        "            \"attachments\": [\n",
        "                {\n",
        "                    \"color\": \"#36a64f\",  # Green color for success\n",
        "                    \"pretext\": \":large_green_circle: Kubeflow Component Alert\",\n",
        "                    \"fields\": [\n",
        "                        {\n",
        "                            \"title\": \"Component Name\",\n",
        "                            \"value\": component_name,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Execution Date\",\n",
        "                            \"value\": execution_date,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Execution Time\",\n",
        "                            \"value\": execution_time,\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Duration\",\n",
        "                            \"value\": f\"{duration} minutes\",\n",
        "                            \"short\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"title\": \"Deployed Endpoint\",\n",
        "                            \"value\": endpoint_name,\n",
        "                            \"short\": True\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(SLACK_WEBHOOK_URL, json=message)\n",
        "            response.raise_for_status()  # Check for request errors\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            pass\n",
        "\n",
        "    # Track the start time of the component execution\n",
        "    start_time = datetime.now()\n",
        "    try:\n",
        "      # Initialize Vertex AI SDK\n",
        "      aiplatform.init(project=project_id, location=location)\n",
        "\n",
        "      # Load the model using the URI\n",
        "      deployed_model = aiplatform.Model(model.uri)\n",
        "\n",
        "      # Check if an endpoint with the given display name exists\n",
        "      endpoints = aiplatform.Endpoint.list(\n",
        "          filter=f'display_name=\"{endpoint_display_name}\"',\n",
        "          order_by=\"create_time desc\",\n",
        "          project=project_id,\n",
        "          location=location\n",
        "      )\n",
        "\n",
        "      if endpoints:\n",
        "          # Reuse the existing endpoint\n",
        "          endpoint_obj = endpoints[0]\n",
        "      else:\n",
        "          # Create a new endpoint\n",
        "          endpoint_obj = aiplatform.Endpoint.create(\n",
        "              display_name=endpoint_display_name\n",
        "          )\n",
        "\n",
        "      # Deploy the model to the endpoint with the given configuration\n",
        "      deployed_model_resource = endpoint_obj.deploy(\n",
        "          model=deployed_model,  # This is now an instantiated Model object\n",
        "          deployed_model_display_name=deployed_model_display_name,\n",
        "          machine_type=\"n1-standard-4\",  # TensorFlow configuration\n",
        "          traffic_split={\"0\": 100},  # Assign 100% traffic to the new deployment\n",
        "          min_replica_count=1,\n",
        "          max_replica_count=3,\n",
        "      )\n",
        "\n",
        "      # Output the endpoint resource name\n",
        "      endpoint.uri = endpoint_obj.resource_name\n",
        "\n",
        "      # Track the end time and calculate duration\n",
        "      end_time = datetime.now()\n",
        "      duration = (end_time - start_time).total_seconds() / 60  # Duration in minutes\n",
        "\n",
        "      send_success_email()\n",
        "      # Send Slack and success email notifications\n",
        "      send_slack_message(\n",
        "          component_name=\"Model Deployment Component\",\n",
        "          execution_date=end_time.strftime('%Y-%m-%d'),\n",
        "          execution_time=end_time.strftime('%H:%M:%S'),\n",
        "          duration=round(duration, 2),\n",
        "          endpoint_name=endpoint_display_name\n",
        "      )\n",
        "\n",
        "    except Exception as e:\n",
        "      # Send failure Slack message and email in case of an error\n",
        "      error_message = str(e)\n",
        "      send_failure_email(error_message)\n",
        "      send_slack_message(\n",
        "          component_name=\"Model Deployment Component Failed\",\n",
        "          execution_date=datetime.now().strftime('%Y-%m-%d'),\n",
        "          execution_time=datetime.now().strftime('%H:%M:%S'),\n",
        "          duration=0,  # If failed, duration is 0\n",
        "          endpoint_name=endpoint_display_name\n",
        "      )\n",
        "\n",
        "    # METADATA INSERTIONS\n",
        "    try:\n",
        "        project_id = \"bilingualcomplaint-system\"\n",
        "        metadata_table = \"bilingualcomplaint-system.MLOps.model_training_metadata\"\n",
        "        preprocessed_data_table = \"bilingualcomplaint-system.MLOps.preprocessed_data\"\n",
        "        # Initialize BigQuery client\n",
        "        bqclient = bigquery.Client(project=project_id)\n",
        "\n",
        "        # Define metadata insertion details\n",
        "        current_timestamp = datetime.utcnow().isoformat()\n",
        "\n",
        "        # Query to get the record count from the preprocessed_data table\n",
        "        query = f\"SELECT COUNT(*) AS record_count FROM `{preprocessed_data_table}`\"\n",
        "        query_job = bqclient.query(query)\n",
        "\n",
        "        # Fetch the query result and process it directly\n",
        "        result = query_job.result()  # This returns a list of Row objects\n",
        "        record_count = None\n",
        "\n",
        "        # Iterate over the results and extract the record count\n",
        "        for row in result:\n",
        "            record_count = int(row[\"record_count\"])\n",
        "\n",
        "        if record_count is None:\n",
        "            raise ValueError(\"No record count returned from BigQuery query.\")\n",
        "\n",
        "        # Prepare data for metadata insertion\n",
        "        rows_to_insert = [\n",
        "            {\n",
        "                \"last_training_timestamp\": current_timestamp,\n",
        "                \"record_count\": record_count\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Insert metadata into BigQuery table\n",
        "        metadata_table = f\"{project_id}.MLOps.model_training_metadata\"\n",
        "        errors = bqclient.insert_rows_json(metadata_table, rows_to_insert)\n",
        "        print(\"hi\")\n",
        "\n",
        "        if errors:\n",
        "            print(f\"Failed to insert metadata: {errors}\")\n",
        "        else:\n",
        "            print(f\"Metadata inserted successfully into {metadata_table}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error inserting metadata into BigQuery: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "57AIkrTKvjdh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@component(\n",
        "    packages_to_install=['google-cloud-aiplatform==1.18.3']\n",
        ")\n",
        "\n",
        "def log_metrics_to_vertex_ai(metrics_dict: Input[Artifact]):\n",
        "    import json\n",
        "    import google.cloud.aiplatform as aiplatform\n",
        "    import time\n",
        "\n",
        "    # Initialize Vertex AI\n",
        "    aiplatform.init(project=\"bilingualcomplaint-system\", location=\"us-east1\", experiment='experiment-demo')\n",
        "\n",
        "    # Read the metrics from the input artifact\n",
        "    with open(metrics_dict.path, 'r') as f:\n",
        "        metrics = json.load(f)\n",
        "\n",
        "    # Log metrics to Vertex AI\n",
        "    run = aiplatform.start_run(\"run-{}\".format(int(time.time())))\n",
        "    aiplatform.log_metrics(metrics)\n",
        "    aiplatform.end_run()\n"
      ],
      "metadata": {
        "id": "-R1UbMAw9m4a"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_name = 'complaint_english'\n",
        "label_name='product'\n",
        "\n",
        "product_labels = ['Credit reporting, credit repair services, or other personal consumer reports',\n",
        " 'Debt collection',\n",
        " 'Checking or savings account',\n",
        " 'Credit card or prepaid card',\n",
        " 'Mortgage',\n",
        " 'Money transfer, virtual currency, or money service',\n",
        " 'Vehicle loan or lease',\n",
        " 'Student loan']\n",
        "\n",
        "label_2_idx_map = {label: idx for idx, label in enumerate(product_labels)}\n",
        "idx_2_label_map = {idx: label for label, idx in label_2_idx_map.items()}"
      ],
      "metadata": {
        "id": "l3F2LEcBBgMD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_params = {}"
      ],
      "metadata": {
        "id": "5qnXlg7_Cnly"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@pipeline(\n",
        "    name=\"product_pipeline_with_bias_and_experimentation_tracking\",\n",
        "    description=\"Model data pipeline in TF\",\n",
        "    pipeline_root=_pipeline_artifacts_dir,\n",
        ")\n",
        "def model_data_pipeline(start_year: int = 2018, end_year: int = 2020):\n",
        "    # Step 1: Get data\n",
        "    get_data_component_task = get_data_component(\n",
        "        project_id=PROJECT_ID,\n",
        "        location=LOCATION,\n",
        "        start_year=start_year,\n",
        "        end_year=end_year,\n",
        "        feature_name='complaints',\n",
        "        label_name='product'\n",
        "    )\n",
        "\n",
        "    # Step 2: Prepare training and validation datasets\n",
        "    train_data_prep_task = prepare_data_component(\n",
        "        data=get_data_component_task.outputs['train_data'],\n",
        "        dataset_name='train',\n",
        "        feature_name='complaints',\n",
        "        label_name='product',\n",
        "        label_map=label_2_idx_map\n",
        "    )\n",
        "    validation_data_prep_task = prepare_data_component(\n",
        "        data=get_data_component_task.outputs['val_data'],\n",
        "        dataset_name='val',\n",
        "        feature_name='complaints',\n",
        "        label_name='product',\n",
        "        label_map=label_2_idx_map\n",
        "    )\n",
        "\n",
        "    # Step 3: Train the mBERT model\n",
        "    train_lstm_task = train_mbert_model(\n",
        "        train_data=train_data_prep_task.outputs['tf_dataset'],\n",
        "        model_params=bert_model_params,\n",
        "        label_map=label_2_idx_map,\n",
        "        train_data_name='train',\n",
        "    )\n",
        "\n",
        "    # Step 4: Test the model\n",
        "    test_task = test_mbert_model(\n",
        "        test_data=validation_data_prep_task.outputs['tf_dataset'],\n",
        "        trained_model=train_lstm_task.outputs['model_output'],\n",
        "        test_data_name='val',\n",
        "        label_map=label_2_idx_map\n",
        "    )\n",
        "    experiment_tracking = log_metrics_to_vertex_ai(\n",
        "      metrics_dict=test_task.outputs['metrics_dict']\n",
        "    )\n",
        "\n",
        "    # Step 5 : Bias detection\n",
        "    bias_detection_task = bias_detection(\n",
        "        test_data=validation_data_prep_task.outputs['tf_dataset'],\n",
        "        trained_model=train_lstm_task.outputs['model_output'],\n",
        "        feature_name='complaints',\n",
        "        label_name='product',\n",
        "        test_data_name='val',\n",
        "        label_map=label_2_idx_map\n",
        "    )\n",
        "    bias_detection_task.after(test_task)\n",
        "\n",
        "    # Step 6: Register the model in the Vertex AI Model Registry\n",
        "    register_model_task = model_registration(\n",
        "        model_output=test_task.outputs['reusable_model'],  # Use trained model output\n",
        "        project_id=PROJECT_ID,\n",
        "        location=LOCATION,\n",
        "        model_display_name=\"tf-mbert-model\",\n",
        "    )\n",
        "    register_model_task.after(bias_detection_task)\n",
        "\n",
        "    # Step 7: Deploy the registered model\n",
        "    deploy_model_task = model_deployment(\n",
        "        model=register_model_task.outputs['model'],  # Use registered model\n",
        "        project_id=PROJECT_ID,\n",
        "        location=LOCATION,\n",
        "        endpoint_display_name=\"tf-mbert-endpoint\",\n",
        "        deployed_model_display_name=\"tf-mbert-deployed\",\n",
        "    )\n",
        "\n",
        "    # Log output for visibility\n",
        "    print(f\"Model Registration: {register_model_task.outputs['model']}\")\n",
        "    print(f\"Model Deployment Endpoint: {deploy_model_task.outputs['endpoint']}\")\n"
      ],
      "metadata": {
        "id": "Cdse-x3Fpr2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b345ff8a-3000-4e40-90b5-207ff630e8b7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Registration: {{channel:task=model-registration;name=model;type=system.Model@0.0.1;}}\n",
            "Model Deployment Endpoint: {{channel:task=model-deployment;name=endpoint;type=system.Artifact@0.0.1;}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compiler.Compiler().compile(\n",
        "    pipeline_func=model_data_pipeline, package_path=\"model_data_pipeline_job.json\"\n",
        ")"
      ],
      "metadata": {
        "id": "9XHVQ9l7bGIV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "job = aiplatform.PipelineJob(\n",
        "    display_name=\"model_mbert_data_pipeline\",\n",
        "    template_path=\"model_data_pipeline_job.json\",\n",
        "    job_id=\"model-data-pipeline-{0}\".format(TIMESTAMP),\n",
        "    enable_caching=True\n",
        ")"
      ],
      "metadata": {
        "id": "6T4ona-ybHkU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job.submit()"
      ],
      "metadata": {
        "id": "3zdI77z-bJVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b054fede-a6e4-494c-bbd2-ed3cf878d21e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/661860051070/locations/us-east1/pipelineJobs/model-data-pipeline-20241129233713\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/661860051070/locations/us-east1/pipelineJobs/model-data-pipeline-20241129233713')\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/us-east1/pipelines/runs/model-data-pipeline-20241129233713?project=661860051070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wcgiaRAWIDiU"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}