# -*- coding: utf-8 -*-
"""MLOps_XLM_RoBERT_KubeFlow_Pipeline_with_Versioning_and_Experimentation_Tracking_Stratified_Working_Product.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qI1go8m-lHRo6nycofxYDMlN0pcXCYZD
"""

!pip install --upgrade pip setuptools

!pip install jedi>=0.16

!pip install kfp

from google.colab import auth
auth.authenticate_user()

import os
import kfp
import json
from datetime import datetime
from kfp import compiler, dsl
from typing import NamedTuple, List, Union, Dict, Any
from google.cloud import aiplatform
from kfp.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics, Dataset
import requests

print(f'KFB version: {kfp.__version__}')

PROJECT_ID = 'bilingualcomplaint-system'
LOCATION = 'us-east1'
# Bucket Name
GCS_artifacts_bucket_name = 'tfx-artifacts'
# Pipeline
pipeline_name = 'complaints-clf-vertex-training'
# Path to various pipeline artifact.
_pipeline_artifacts_dir = f'gs://{GCS_artifacts_bucket_name}/pipeline_artifacts/{pipeline_name}'

aiplatform.init(
    project=PROJECT_ID,
    location=LOCATION,
    staging_bucket=f'gs://{GCS_artifacts_bucket_name}',
    )

@component(
    base_image="python:3.10-slim",
    packages_to_install = [
        'google-cloud-bigquery==3.26.0',
        'pandas==1.5.3',
        'numpy==1.26.4',
        'db-dtypes==1.3.0',
        'scikit-learn==1.5.2'
        ]
    )
def get_data_component(
    project_id: str,
    location: str,
    start_year: int, end_year: int,
    feature_name: str,
    label_name: str,
    train_data: Output[Dataset],
    val_data: Output[Dataset],
    testset_size: float = 0.2,
    limit:int=100):

  from google.cloud import bigquery
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import LabelEncoder
  import requests
  from datetime import datetime
  import smtplib
  from email.mime.text import MIMEText
  from email.mime.multipart import MIMEMultipart

  # Track the start time of the component execution
  start_time = datetime.now()

  # Function to send custom Slack message with Kubeflow component details
  def send_slack_message(component_name, execution_date, execution_time, duration):
      SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'  # Replace with your Slack webhook URL
      message = {
          "attachments": [
              {
                  "color": "#36a64f",  # Green color for success
                  "pretext": ":large_green_circle: Kubeflow Component Success Alert",
                  "fields": [
                      {
                          "title": "Component Name",
                          "value": component_name,
                          "short": True
                      },
                      {
                          "title": "Execution Date",
                          "value": execution_date,
                          "short": True
                      },
                      {
                          "title": "Execution Time",
                          "value": execution_time,
                          "short": True
                      },
                      {
                          "title": "Duration",
                          "value": f"{duration} minutes",
                          "short": True
                      }
                  ]
              }
          ]
      }

      try:
          response = requests.post(SLACK_WEBHOOK_URL, json=message)
          response.raise_for_status()  # Check for request errors
          pass
      except requests.exceptions.RequestException as e:
          pass

  # Function to send success email
  def send_success_email():
      sender_email = "sucessemailtrigger@gmail.com"
      password = "jomnpxbfunwjgitb"
      receiver_emails = ["hegde.anir@northeastern.edu",
                         "nenavath.r@northeastern.edu",
                         "pandey.raj@northeastern.edu",
                         "khatri.say@northeastern.edu",
                         "singh.arc@northeastern.edu",
                         "goparaju.v@northeastern.edu"]

      # Current time for logging purposes
      current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

      # Create the email content
      subject = '[Kubeflow Pipeline] - Started'
      body = f'''Hi team,

      Model training in the Kubeflow pipeline has started!

      Details:
      - Start Time: {current_time}
      - Dataset: {start_year}-{end_year}

      Please monitor the pipeline for further updates.
      '''

      try:
          # Set up the SMTP server
          server = smtplib.SMTP('smtp.gmail.com', 587)
          server.starttls()  # Secure the connection
          server.login(sender_email, password)

          # Send email to each receiver
          for receiver_email in receiver_emails:
              # Create a fresh message for each recipient
              message = MIMEMultipart()
              message['From'] = sender_email
              message['To'] = receiver_email
              message['Subject'] = subject
              message.attach(MIMEText(body, 'plain'))

              # Send the email
              server.sendmail(sender_email, receiver_email, message.as_string())

      except Exception as e:
          pass
      finally:
          server.quit()

  # Function to send failure email
  def send_failure_email(error_message):
      sender_email = "sucessemailtrigger@gmail.com"
      password = "jomnpxbfunwjgitb"
      receiver_emails = ["hegde.anir@northeastern.edu",
                         "nenavath.r@northeastern.edu",
                         "pandey.raj@northeastern.edu",
                         "khatri.say@northeastern.edu",
                         "singh.arc@northeastern.edu",
                         "goparaju.v@northeastern.edu"]

      # Create the email content
      subject = '[Kubeflow Pipeline]'
      body = f'Hi team,\nModel training has failed!.\nError Details: {error_message}'

      try:
          # Set up the SMTP server
          server = smtplib.SMTP('smtp.gmail.com', 587)
          server.starttls()  # Secure the connection
          server.login(sender_email, password)

          # Send email to each receiver
          for receiver_email in receiver_emails:
              # Create a fresh message for each recipient
              message = MIMEMultipart()
              message['From'] = sender_email
              message['To'] = receiver_email
              message['Subject'] = subject
              message.attach(MIMEText(body, 'plain'))

              # Send the email
              server.sendmail(sender_email, receiver_email, message.as_string())

      except Exception as e:
          pass
      finally:
          server.quit()

  try:
    bqclient = bigquery.Client(project=project_id, location=location)

    QUERY = f'''select complaint_english, complaint_hindi, product
    from (
    select *, row_number() over(partition by product) as rank_order
    from `bilingualcomplaint-system.MLOps`.get_dataset_by_complaint_year_interval(2018, 2023)
    where 1=1
    and product in (
    select product
    from `bilingualcomplaint-system.MLOps`.get_dataset_by_complaint_year_interval(2018, 2023)
    group by product
    having count(product) >= 681 ) )
    where rank_order <= 681
    '''
    query_job = bqclient.query(QUERY)  # API request
    rows = query_job.result()  # Waits for query to finish
    data = rows.to_dataframe()

    products = data['product'].tolist()
    # Encode product labels
    product_encoder = LabelEncoder()
    encoded_products  = product_encoder.fit_transform(products)
    num_labels = len(product_encoder.classes_)
    # Melt the DataFrame
    data_features = data.melt(
        id_vars=["product"],  # Columns to keep as-is
        value_vars=["complaint_english", "complaint_hindi"],  # Columns to melt
        value_name="complaints"  # New column name for melted values
    )

    train, val = train_test_split(data_features[[feature_name, label_name]], test_size=testset_size, stratify=data_features[label_name], random_state=42)
    train.reset_index(drop=True, inplace=True)
    val.reset_index(drop=True, inplace=True)
    train.to_pickle(train_data.path)
    val.to_pickle(val_data.path)
    # Track the end time and calculate duration
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds() / 60  # Duration in minutes

    # Send success email once the data is processed
    send_success_email()
    # Send the Slack message with execution details
    send_slack_message(
        component_name="Getting Data Component",
        execution_date=end_time.strftime('%Y-%m-%d'),
        execution_time=end_time.strftime('%H:%M:%S'),
        duration=round(duration, 2)  # Round duration to 2 decimal places
    )

  except Exception as e:
      # Send failure email if there's an error
      error_message = str(e)
      send_failure_email(error_message)
      send_slack_message(
          component_name="Model Training Component Failed",
          execution_date=datetime.now().strftime('%Y-%m-%d'),
          execution_time=datetime.now().strftime('%H:%M:%S'),
          duration=0  # If failed, duration is 0
      )

@component(
    base_image="tensorflow/tensorflow:2.14.0",
    packages_to_install = [
        'pandas==1.5.3',
        'numpy==1.26.4',
        'transformers==4.44.2',
        'sentencepiece'
        ]
    )
def prepare_data_component(
    data: Input[Dataset],
    tf_dataset: Output[Dataset],
    dataset_name: str,
    feature_name: str, label_name: str,
    label_map: Dict[str, int],
    batch_size:int = 32,
    max_sequence_length: int = 128,
    hugging_face_model_name: str = 'xlm-roberta-base'
    ):
  import os
  import pickle
  import pandas as pd
  import tensorflow as tf
  from transformers import XLMRobertaTokenizer


  # Function to serialize each example
  def serialize_example(feature, label):
    feature = tf.train.Feature(int64_list=tf.train.Int64List(value=feature))
    label = tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))
    feature_dict = {
        'feature': feature,
        'label': label
    }
    example_proto = tf.train.Example(features=tf.train.Features(feature=feature_dict))
    return example_proto.SerializeToString()


  # Load the XLM-R tokenizer
  tokenizer = XLMRobertaTokenizer.from_pretrained(hugging_face_model_name)


  # Load the pickled dataset
  with open(data.path, 'rb') as f:
    loaded_dataframe: pd.DataFrame  = pickle.load(f)

  # Create a TextVectorization layer
  max_features = 1000  # Adjust based on your vocabulary size
  sequence_length = 450  # Adjust based on your text length

  vectorize_layer = tf.keras.layers.TextVectorization(
      standardize="lower_and_strip_punctuation",
      max_tokens=max_features,
      output_mode='int',
      output_sequence_length=sequence_length)

  # Adapt the layer to your text data
  vectorize_layer.adapt(loaded_dataframe[feature_name])


  # Create a dictionary to map each unique label in 'product' to an integer
  labels = loaded_dataframe[label_name].map(label_map).values

  # Convert labels to tensor format for compatibility with TensorFlow
  labels_tf = tf.convert_to_tensor(labels, dtype=tf.int32)

  features_tf = tokenizer(
      loaded_dataframe[feature_name].tolist(),
      padding=True, truncation=True,
      return_tensors="tf",
      max_length=max_sequence_length)["input_ids"]

  # Create TensorFlow datasets
  tf_prepared_dataset = tf.data.Dataset.from_tensor_slices((features_tf, labels_tf))

  # make the folder if does not exist
  os.makedirs(tf_dataset.path, exist_ok=True)

  tfrecord_file_path = os.path.join(tf_dataset.path, f'{dataset_name}.tfrecord')

  # Write the dataset to TFRecord
  with tf.io.TFRecordWriter(tfrecord_file_path) as writer:
    for feature, label in tf_prepared_dataset:
      serialized_example = serialize_example(feature, label)
      writer.write(serialized_example)

@component(
    base_image="tensorflow/tensorflow:2.14.0",
    packages_to_install = [
        'pandas==1.5.3',
        'numpy==1.26.4',
        'transformers==4.44.2',
        'keras==2.14.0',
        'sentencepiece'
        ]
    )
def train_mbert_model(
    train_data: Input[Dataset],
    model_output: Output[Model],
    train_data_name: str,
    model_params: Dict[str, Any],
    label_map: Dict[str, int],
    model_save_name: str = 'saved_xlmroberta_product_model',
    huggingface_model_name: str = 'xlm-roberta-base',
    max_epochs: int = 2,
    train_size: float = 0.8,
    batch_size: int = 8,
    max_sequence_length: int = 128,
    ):
  import os
  import tensorflow as tf
  from transformers import TFXLMRobertaForSequenceClassification, XLMRobertaTokenizer
  from datetime import datetime
  import requests

  # Track the start time of the component execution
  start_time = datetime.now()

  # Function to send custom Slack message with Kubeflow component details
  def send_slack_message(component_name, execution_date, execution_time, duration):
      SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'  # Replace with your Slack webhook URL
      message = {
          "attachments": [
              {
                  "color": "#36a64f",  # Green color for success
                  "pretext": ":large_green_circle: Kubeflow Component Success Alert",
                  "fields": [
                      {
                          "title": "Component Name",
                          "value": component_name,
                          "short": True
                      },
                      {
                          "title": "Execution Date",
                          "value": execution_date,
                          "short": True
                      },
                      {
                          "title": "Execution Time",
                          "value": execution_time,
                          "short": True
                      },
                      {
                          "title": "Duration",
                          "value": f"{duration} minutes",
                          "short": True
                      }
                  ]
              }
          ]
      }

      try:
          response = requests.post(SLACK_WEBHOOK_URL, json=message)
          response.raise_for_status()  # Check for request errors
          pass
      except requests.exceptions.RequestException as e:
          pass

  def parse_tfrecord_fn(example_proto, max_sequence_length=max_sequence_length):
    # Define the feature description dictionary
    feature_description = {
        'feature': tf.io.FixedLenFeature([max_sequence_length], tf.int64),  # Adjust the shape [128] as per your data
        'label': tf.io.FixedLenFeature([], tf.int64),
        }
    # Parse the input tf.train.Example proto using the feature description
    parsed_example = tf.io.parse_single_example(example_proto, feature_description)
    return parsed_example['feature'], parsed_example['label']

  def build_bert_model(vocab_size, embedding_dim=128, lstm_units=64, num_classes=2):
      model = TFXLMRobertaForSequenceClassification.from_pretrained(
          huggingface_model_name,
          num_labels=num_classes
      )
      return model

  try:

    tokenizer = XLMRobertaTokenizer.from_pretrained(huggingface_model_name)
    vocab_size = tokenizer.vocab_size
    record_path = os.path.join(train_data.path, f'{train_data_name}.tfrecord')
    train_dataset = tf.data.TFRecordDataset(record_path)
    train_dataset = train_dataset.map(parse_tfrecord_fn)
    train_dataset = train_dataset.cache()

    # Determine the total number of elements in the train_dataset
    total_size = len(list(train_dataset.as_numpy_iterator()))
    train_size = int(train_size * total_size)
    val_size = total_size - train_size

    # Split the dataset
    train_subset = train_dataset.take(train_size)
    validation_subset = train_dataset.skip(train_size)

    train_subset = train_subset.batch(batch_size).shuffle(buffer_size=train_size)
    validation_subset = validation_subset.batch(batch_size)

    model = build_bert_model(
        vocab_size=vocab_size,
        embedding_dim=model_params.get('embed_dim', 128),
        lstm_units=model_params.get('hidden_dim', 128),
        num_classes=len(label_map)
    )

    optimizer = tf.optimizers.Adam(learning_rate=3e-5)
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    model.compile(
        optimizer=optimizer,
        loss=loss,
        metrics=['accuracy']
        )

    # Define EarlyStopping and ReduceLROnPlateau callbacks
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=3,
        restore_best_weights=True
    )

    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.1,
        patience=3,
        min_lr=1e-6
    )

    # Define TensorBoard callback for logging
    log_dir = os.path.join(model_output.path, 'logs')
    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)

    history = model.fit(
          train_subset,
          validation_data=validation_subset,
          epochs=max_epochs,
          callbacks=[early_stopping, lr_scheduler, tensorboard_callback]
          )

    os.makedirs(model_output.path, exist_ok=True)

    model.save(os.path.join(model_output.path, f'{model_save_name}.keras'))
    model.save(os.path.join(model_output.path))

    # Track the end time and calculate duration
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds() / 60  # Duration in minutes

    # Send the Slack message with execution details
    send_slack_message(
        component_name="Model Training Component",
        execution_date=end_time.strftime('%Y-%m-%d'),
        execution_time=end_time.strftime('%H:%M:%S'),
        duration=round(duration, 2)  # Round duration to 2 decimal places
    )

  except Exception as e:
      error_message = str(e)
      print(f"Error during model training: {error_message}")
      send_slack_message(
          component_name="Model Training Component",
          execution_date=datetime.now().strftime('%Y-%m-%d'),
          execution_time=datetime.now().strftime('%H:%M:%S'),
          duration=0  # If failed, duration is 0
      )

# TEST M-BERT component
@component(
    base_image="tensorflow/tensorflow:2.14.0",
    packages_to_install = [
        'pandas==1.5.3',
        'numpy==1.26.4',
        'transformers==4.44.2',
        'scikit-learn==1.5.2',
        'keras==2.14.0',
        'sentencepiece'

    ]
)
def test_mbert_model(
    test_data: Input[Dataset],
    trained_model: Input[Model],
    metric: Output[Metrics],
    reusable_model: Output[Model],  # Reference trained model path
    metrics_dict: Output[Artifact],
    test_data_name: str,
    label_map: Dict[str, int],
    model_name: str = 'saved_xlmroberta_product_model',
    batch_size: int = 8,
    max_sequence_length: int = 128,
    huggingface_model_name: str = 'xlm-roberta-base'
):
    import os
    import tensorflow as tf
    from sklearn.metrics import precision_score, recall_score, f1_score
    # Load the XLM-R model for sequence classification
    from transformers import TFXLMRobertaForSequenceClassification
    import requests
    from datetime import datetime
    import json

    # Track the start time of the component execution
    start_time = datetime.now()
    # Function to send custom Slack message with Kubeflow component details
    def send_slack_message(component_name, execution_date, execution_time, duration, f1_score=None, precision=None, recall=None):
        SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'
        message = {
            "attachments": [
                {
                    "color": "#36a64f",  # Green color for success
                    "pretext": ":large_green_circle: Kubeflow Component Success Alert",
                    "fields": [
                        {
                            "title": "Component Name",
                            "value": component_name,
                            "short": True
                        },
                        {
                            "title": "Execution Date",
                            "value": execution_date,
                            "short": True
                        },
                        {
                            "title": "Execution Time",
                            "value": execution_time,
                            "short": True
                        },
                        {
                            "title": "Duration",
                            "value": f"{duration} minutes",
                            "short": True
                        }
                    ]
                }
            ]
        }

        if f1_score is not None:
            message["attachments"][0]["fields"].append({
                "title": "Validation F1 Score",
                "value": f"{f1_score:.4f}",
                "short": True
            })

        if precision is not None:
            message["attachments"][0]["fields"].append({
                "title": "Validation Precision",
                "value": f"{precision:.4f}",
                "short": True
            })

        if recall is not None:
            message["attachments"][0]["fields"].append({
                "title": "Validation Recall",
                "value": f"{recall:.4f}",
                "short": True
            })

        try:
            response = requests.post(SLACK_WEBHOOK_URL, json=message)
            response.raise_for_status()  # Check for request errors
        except requests.exceptions.RequestException as e:
          pass

    def parse_tfrecord_fn(example_proto, max_sequence_length=max_sequence_length):
      feature_description = {
          'feature': tf.io.FixedLenFeature([max_sequence_length], tf.int64),
          'label': tf.io.FixedLenFeature([], tf.int64),
      }
      parsed_example = tf.io.parse_single_example(example_proto, feature_description)
      return parsed_example['feature'], parsed_example['label']

    try:
      # Load test dataset
      record_path = os.path.join(test_data.path, f'{test_data_name}.tfrecord')
      test_dataset = tf.data.TFRecordDataset(record_path)
      test_dataset = test_dataset.map(parse_tfrecord_fn).batch(batch_size)

      # Load model from the trained model path
      model_path = os.path.join(trained_model.path, f'{model_name}.keras')
      model = TFXLMRobertaForSequenceClassification.from_pretrained(
          huggingface_model_name,
          num_labels=len(label_map)
      )
      model.load_weights(model_path)

      ########model = tf.keras.models.load_model(trained_model.path)  # Load the model in SavedModel format

      # Get predictions
      predictions = model.predict(test_dataset)
      logits = predictions.logits

      # Calculate metrics
      y_true = [label for _, label in list(test_dataset.unbatch().as_numpy_iterator())]
      y_pred = list(tf.argmax(logits, axis=1).numpy())

      precision = precision_score(y_true, y_pred, average='weighted')
      recall = recall_score(y_true, y_pred, average='weighted')
      f1 = f1_score(y_true, y_pred, average='weighted')

      # Calculate per-label metrics
      precision_per_label = precision_score(y_true, y_pred, average=None)
      recall_per_label = recall_score(y_true, y_pred, average=None)
      f1_per_label = f1_score(y_true, y_pred, average=None)


      metric.log_metric('precision', precision)
      metric.log_metric('recall', recall)
      metric.log_metric('f1', f1)
      metrics = {'precision': precision, 'recall': recall, 'f1': f1}

      # Reference the trained model path
      reusable_model.uri = trained_model.uri  # Reuse the same path

      print(f'Precision: {precision:.4f}')
      print(f'Recall: {recall:.4f}')
      print(f'F1 Score: {f1:.4f}')
      print(f'Referenced trained model path: {reusable_model.uri}')

      # Ensure the directory exists
      os.makedirs(os.path.dirname(metrics_dict.path), exist_ok=True)

      # Save metrics to an output artifact
      metrics = {
      'precision_product': precision,
      'recall_product': recall,
      'f1_product': f1,
      'precision_per_label_product': json.dumps(precision_per_label.tolist()),
      'recall_per_label_product': json.dumps(recall_per_label.tolist()),
      'f1_per_label_product': json.dumps(f1_per_label.tolist())
  }

      with open(metrics_dict.path, 'w') as f:
          json.dump(metrics, f)

      # Track the end time and calculate duration
      end_time = datetime.now()
      duration = (end_time - start_time).total_seconds() / 60  # Duration in minutes

      # Send the Slack message with execution details and metrics
      send_slack_message(
          component_name="Model Testing Component",
          execution_date=end_time.strftime('%Y-%m-%d'),
          execution_time=end_time.strftime('%H:%M:%S'),
          duration=round(duration, 2),
          f1_score=f1,
          precision=precision,
          recall=recall
      )

    except Exception as e:
      error_message = str(e)
      print(f"Error during model testing: {error_message}")
      send_slack_message(
          component_name="Model Testing Component Failed",
          execution_date=datetime.now().strftime('%Y-%m-%d'),
          execution_time=datetime.now().strftime('%H:%M:%S'),
          duration= 0  # If failed, duration is 0
      )
      raise e

@component(
    base_image="python:3.10-slim",
    packages_to_install=[
        'pandas==1.5.3',
        'numpy==1.26.4',
        'tensorflow==2.14.0',
        'fairlearn==0.8.0',
        'scikit-learn==1.5.2',
        'transformers==4.44.2',
        'json5',
        'sentencepiece'

    ]
)
def bias_detection(
    test_data: Input[Dataset],
    trained_model: Input[Model],
    bias_report: Output[Dataset],
    feature_name: str,
    label_name: str,
    test_data_name: str,
    label_map: Dict[str, int],
    model_name: str = 'saved_xlmroberta_product_model',
    batch_size: int = 8,
    max_sequence_length: int = 128,
    huggingface_model_name: str = 'xlm-roberta-base',
    accuracy_threshold: float = 0.2
):
    import os
    import pandas as pd
    import tensorflow as tf
    from fairlearn.metrics import MetricFrame, true_positive_rate, false_positive_rate, selection_rate
    from transformers import TFXLMRobertaForSequenceClassification
    import requests
    import json
    from datetime import datetime

    # Hardcoded label map
    idx_2_label_map = {
        0: 'Credit reporting, credit repair services, or other personal consumer reports',
        1: 'Debt collection',
        2: 'Checking or savings account',
        3: 'Credit card or prepaid card',
        4: 'Mortgage',
        6: 'Vehicle loan or lease',
        7: 'Student loan'
    }

    # Function to send alert
    def send_alert(summary_message):
        SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'
        payload = {"text": summary_message}
        try:
            response = requests.post(SLACK_WEBHOOK_URL, json=payload)
            response.raise_for_status()
            print(f"Alert sent successfully: {summary_message}")
        except Exception as e:
            print(f"Failed to send alert: {str(e)}")

    print(f"Starting bias detection component...")
    print(f"Test data path: {test_data.path}")
    print(f"Trained model path: {trained_model.path}")

          # Load and parse test dataset
    def parse_tfrecord_fn(example_proto, max_sequence_length=max_sequence_length):
      feature_description = {
          'feature': tf.io.FixedLenFeature([max_sequence_length], tf.int64),
          'label': tf.io.FixedLenFeature([], tf.int64),
      }
      parsed_example = tf.io.parse_single_example(example_proto, feature_description)
      return parsed_example['feature'], parsed_example['label']


    try:
        print(f"Loading test dataset...")
        record_path = os.path.join(test_data.path, f'{test_data_name}.tfrecord')
        print(f"Record path: {record_path}")
        test_dataset = tf.data.TFRecordDataset(record_path)
        test_dataset = test_dataset.map(parse_tfrecord_fn).batch(batch_size)

        print(f"Loading trained model...")
        model_path = os.path.join(trained_model.path, f'{model_name}.keras')
        print(f"Model path: {model_path}")
        model = TFXLMRobertaForSequenceClassification.from_pretrained(
            huggingface_model_name,
            num_labels=len(label_map)
        )
        model.load_weights(model_path)

        print(f"Making predictions...")
        y_true, y_pred = [], []
        for features, labels in test_dataset.unbatch().as_numpy_iterator():
            logits = model(tf.expand_dims(features, axis=0)).logits
            predicted_label = tf.argmax(logits, axis=1).numpy()[0]

            y_true.append(labels)
            y_pred.append(predicted_label)

        print(f"Building DataFrame for Fairlearn slicing...")
        df = pd.DataFrame({
            'true_label': y_true,
            'predicted_label': y_pred
        })


        # Define multiclass metrics
        def multiclass_metric(metric_fn, y_true, y_pred, label):
            """Compute a metric for a specific label in a one-vs-rest approach."""
            y_true_binary = (y_true == label).astype(int)
            y_pred_binary = (y_pred == label).astype(int)
            return metric_fn(y_true_binary, y_pred_binary)

        metrics = {
            'accuracy': lambda y_true, y_pred: (y_true == y_pred).mean(),
            'true_positive_rate': lambda y_true, y_pred: {
                label: multiclass_metric(true_positive_rate, y_true, y_pred, label)
                for label in set(y_true)
            },
            'false_positive_rate': lambda y_true, y_pred: {
                label: multiclass_metric(false_positive_rate, y_true, y_pred, label)
                for label in set(y_true)
            },
            'selection_rate': lambda y_true, y_pred: {
                label: multiclass_metric(selection_rate, y_true, y_pred, label)
                for label in set(y_true)
            },
        }

        # Calculate metrics using Fairlearn
        print("Calculating metrics using Fairlearn...")
        metric_frame = MetricFrame(
            metrics=metrics,
            y_true=df['true_label'],
            y_pred=df['predicted_label'],
            sensitive_features=df['true_label']
        )
        bias_report_df = metric_frame.by_group.reset_index()

        # Identify slices with low accuracy
        print(f"Checking for accuracy below threshold ({accuracy_threshold})...")
        grouping_column = 'true_label' if 'true_label' in bias_report_df.columns else bias_report_df.columns[0]
        low_accuracy_slices = bias_report_df[bias_report_df['accuracy'] < accuracy_threshold]

        # Generate alert if necessary
        if not low_accuracy_slices.empty:
            low_accuracy_slices_str = "\n".join([
                f"- {idx_2_label_map.get(row[grouping_column], 'Unknown')}: Accuracy {row['accuracy']:.2f}"
                for _, row in low_accuracy_slices.iterrows()
            ])
            summary_message = (
                f"Bias Alert: The following slices have accuracy below the threshold of {accuracy_threshold:.2f}:\n"
                f"{low_accuracy_slices_str}\n\n"
                f"**Action Required:** Please mitigate this issue as soon as possible to ensure fairness in the model."
            )
            print(summary_message)
            send_alert(summary_message)
        else:
            print("No bias detected. All slices meet the accuracy threshold.")

        print("Bias detection completed.")

    except Exception as e:
        print(f"Error in bias detection: {str(e)}")
        raise e

# metrics = {
        #     'accuracy': lambda y_true, y_pred: (y_true == y_pred).mean(),
        #     'true_positive_rate': lambda y_true, y_pred: {
        #         label: true_positive_rate((y_true == label).astype(int), (y_pred == label).astype(int))
        #         for label in set(y_true)
        #     },
        #     'false_positive_rate': lambda y_true, y_pred: {
        #         label: false_positive_rate((y_true == label).astype(int), (y_pred == label).astype(int))
        #         for label in set(y_true)
        #     },
        #     'selection_rate': lambda y_true, y_pred: {
        #         label: selection_rate((y_true == label).astype(int), (y_pred == label).astype(int))
        #         for label in set(y_true)
        #     },
        # }

        # metric_frame = MetricFrame(metrics=metrics, y_true=df['true_label'], y_pred=df['predicted_label'], sensitive_features=df['true_label'])
        # bias_report_df = metric_frame.by_group.reset_index()

        # bias_report.uri = f"gs://bias_reports/bias_report.csv"

@component(
  packages_to_install=["google-cloud-aiplatform", "google-auth"]
)
def model_registration(
  model_output: Input[Model],
  project_id: str,
  location: str,
  model_display_name: str,
  model: Output[Model]
):
  from google.cloud import aiplatform

  # Initialize Vertex AI SDK
  aiplatform.init(project=project_id, location=location)

  # Use the URI directly without appending "/model"
  model_uri = model_output.uri

  # Check if a model with the same display name already exists
  existing_models = aiplatform.Model.list(
      filter=f'display_name="{model_display_name}"',
      order_by="create_time desc",
      project=project_id,
      location=location
  )

  if existing_models:
      # Get the first existing model's resource name (ID) to add a new version under it
      parent_model = existing_models[0]
      model_id = parent_model.resource_name

      # Register the model under the existing model ID
      registered_model = aiplatform.Model.upload(
          display_name=model_display_name,
          artifact_uri=model_uri,
          serving_container_image_uri="us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest",  # TensorFlow Serving Image
          parent_model=model_id  # Use the existing model's ID to create a new version
      )
  else:
      # No existing model, create a new one
      registered_model = aiplatform.Model.upload(
          display_name=model_display_name,
          artifact_uri=model_uri,
          serving_container_image_uri="us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest"  # TensorFlow Serving Image
      )

  # Output the model resource name
  model.uri = registered_model.resource_name

@component(
    packages_to_install=["google-cloud-aiplatform"]
)
def model_deployment(
    model: Input[Model],  # This is a string (model URI)
    project_id: str,
    location: str,
    endpoint_display_name: str,
    deployed_model_display_name: str,
    endpoint: Output[Artifact]
):
    from google.cloud import aiplatform, bigquery
    import requests
    from datetime import datetime
    from email.mime.multipart import MIMEMultipart
    from email.mime.text import MIMEText
    import smtplib

    # Function to send success email
    def send_success_email():
        sender_email = "sucessemailtrigger@gmail.com"
        password = "jomnpxbfunwjgitb"
        receiver_emails = ["hegde.anir@northeastern.edu",
                          "nenavath.r@northeastern.edu",
                          "pandey.raj@northeastern.edu",
                          "khatri.say@northeastern.edu",
                          "singh.arc@northeastern.edu",
                          "goparaju.v@northeastern.edu"]


        # Current time for logging purposes
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        # Create the email content
        subject = '[Kubeflow Pipeline] - Completed'
        body = f'''Hi team,

        Model has been deployed!

        Details:
        - Start Time: {current_time}


        '''

        try:
            # Set up the SMTP server
            server = smtplib.SMTP('smtp.gmail.com', 587)
            server.starttls()  # Secure the connection
            server.login(sender_email, password)

            # Send email to each receiver
            for receiver_email in receiver_emails:
                # Create a fresh message for each recipient
                message = MIMEMultipart()
                message['From'] = sender_email
                message['To'] = receiver_email
                message['Subject'] = subject
                message.attach(MIMEText(body, 'plain'))

                # Send the email
                server.sendmail(sender_email, receiver_email, message.as_string())

        except Exception as e:
            pass
        finally:
            server.quit()

    # Function to send failure email
    def send_failure_email(error_message):
        sender_email = "sucessemailtrigger@gmail.com"
        password = "jomnpxbfunwjgitb"
        receiver_emails = ["hegde.anir@northeastern.edu",
                          "nenavath.r@northeastern.edu",
                          "pandey.raj@northeastern.edu",
                          "khatri.say@northeastern.edu",
                          "singh.arc@northeastern.edu",
                          "goparaju.v@northeastern.edu"]

        # Create the email content
        subject = '[Kubeflow Pipeline]'
        body = f'Hi team,\nModel deployment has failed!.\nError Details: {error_message}'

        try:
            # Set up the SMTP server
            server = smtplib.SMTP('smtp.gmail.com', 587)
            server.starttls()  # Secure the connection
            server.login(sender_email, password)

            # Send email to each receiver
            for receiver_email in receiver_emails:
                # Create a fresh message for each recipient
                message = MIMEMultipart()
                message['From'] = sender_email
                message['To'] = receiver_email
                message['Subject'] = subject
                message.attach(MIMEText(body, 'plain'))

                # Send the email
                server.sendmail(sender_email, receiver_email, message.as_string())

        except Exception as e:
            pass
        finally:
            server.quit()

    # Function to send custom Slack message with Kubeflow component details
    def send_slack_message(component_name, execution_date, execution_time, duration, endpoint_name):
        SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'  # Replace with your Slack webhook URL
        message = {
            "attachments": [
                {
                    "color": "#36a64f",  # Green color for success
                    "pretext": ":large_green_circle: Kubeflow Component Alert",
                    "fields": [
                        {
                            "title": "Component Name",
                            "value": component_name,
                            "short": True
                        },
                        {
                            "title": "Execution Date",
                            "value": execution_date,
                            "short": True
                        },
                        {
                            "title": "Execution Time",
                            "value": execution_time,
                            "short": True
                        },
                        {
                            "title": "Duration",
                            "value": f"{duration} minutes",
                            "short": True
                        },
                        {
                            "title": "Deployed Endpoint",
                            "value": endpoint_name,
                            "short": True
                        }
                    ]
                }
            ]
        }

        try:
            response = requests.post(SLACK_WEBHOOK_URL, json=message)
            response.raise_for_status()  # Check for request errors
        except requests.exceptions.RequestException as e:
            pass

    # Track the start time of the component execution
    start_time = datetime.now()
    try:
      # Initialize Vertex AI SDK
      aiplatform.init(project=project_id, location=location)

      # Load the model using the URI
      deployed_model = aiplatform.Model(model.uri)

      # Check if an endpoint with the given display name exists
      endpoints = aiplatform.Endpoint.list(
          filter=f'display_name="{endpoint_display_name}"',
          order_by="create_time desc",
          project=project_id,
          location=location
      )

      if endpoints:
          # Reuse the existing endpoint
          endpoint_obj = endpoints[0]
      else:
          # Create a new endpoint
          endpoint_obj = aiplatform.Endpoint.create(
              display_name=endpoint_display_name
          )

      # Deploy the model to the endpoint with the given configuration
      deployed_model_resource = endpoint_obj.deploy(
          model=deployed_model,  # This is now an instantiated Model object
          deployed_model_display_name=deployed_model_display_name,
          machine_type="n1-standard-4",  # TensorFlow configuration
          traffic_split={"0": 100},  # Assign 100% traffic to the new deployment
          min_replica_count=1,
          max_replica_count=3,
      )

      # Output the endpoint resource name
      endpoint.uri = endpoint_obj.resource_name

      # Track the end time and calculate duration
      end_time = datetime.now()
      duration = (end_time - start_time).total_seconds() / 60  # Duration in minutes

      send_success_email()
      # Send Slack and success email notifications
      send_slack_message(
          component_name="Model Deployment Component",
          execution_date=end_time.strftime('%Y-%m-%d'),
          execution_time=end_time.strftime('%H:%M:%S'),
          duration=round(duration, 2),
          endpoint_name=endpoint_display_name
      )

    except Exception as e:
      # Send failure Slack message and email in case of an error
      error_message = str(e)
      send_failure_email(error_message)
      send_slack_message(
          component_name="Model Deployment Component Failed",
          execution_date=datetime.now().strftime('%Y-%m-%d'),
          execution_time=datetime.now().strftime('%H:%M:%S'),
          duration=0,  # If failed, duration is 0
          endpoint_name=endpoint_display_name
      )

    # METADATA INSERTIONS
    try:
        project_id = "bilingualcomplaint-system"
        metadata_table = "bilingualcomplaint-system.MLOps.model_training_metadata"
        preprocessed_data_table = "bilingualcomplaint-system.MLOps.preprocessed_data"
        # Initialize BigQuery client
        bqclient = bigquery.Client(project=project_id)

        # Define metadata insertion details
        current_timestamp = datetime.utcnow().isoformat()

        # Query to get the record count from the preprocessed_data table
        query = f"SELECT COUNT(*) AS record_count FROM `{preprocessed_data_table}`"
        query_job = bqclient.query(query)

        # Fetch the query result and process it directly
        result = query_job.result()  # This returns a list of Row objects
        record_count = None

        # Iterate over the results and extract the record count
        for row in result:
            record_count = int(row["record_count"])

        if record_count is None:
            raise ValueError("No record count returned from BigQuery query.")

        # Prepare data for metadata insertion
        rows_to_insert = [
            {
                "last_training_timestamp": current_timestamp,
                "record_count": record_count
            }
        ]

        # Insert metadata into BigQuery table
        metadata_table = f"{project_id}.MLOps.model_training_metadata"
        errors = bqclient.insert_rows_json(metadata_table, rows_to_insert)
        print("hi")

        if errors:
            print(f"Failed to insert metadata: {errors}")
        else:
            print(f"Metadata inserted successfully into {metadata_table}")

    except Exception as e:
        print(f"Error inserting metadata into BigQuery: {str(e)}")

@component(
    packages_to_install=['google-cloud-aiplatform==1.18.3']
)

def log_metrics_to_vertex_ai(metrics_dict: Input[Artifact]):
    import json
    import google.cloud.aiplatform as aiplatform
    import time

    # Initialize Vertex AI
    aiplatform.init(project="bilingualcomplaint-system", location="us-east1", experiment='experiment-demo')


    # Read the metrics from the input artifact
    with open(metrics_dict.path, 'r') as f:
        metrics = json.load(f)

    # Log metrics to Vertex AI
    run = aiplatform.start_run("run-{}".format(int(time.time())))
    aiplatform.log_metrics(metrics)
    aiplatform.end_run()

feature_name = 'complaint_english'
label_name='product'

product_labels = ['checking or savings account','credit / debt management & repair services', 'credit / prepaid card services','money transfers','mortgage','payday loan','student loan','vehicle loan']
label_2_idx_map = {label: idx for idx, label in enumerate(product_labels)}
idx_2_label_map = {idx: label for label, idx in label_2_idx_map.items()}

bert_model_params = {}

@pipeline(
    name="product_stratify_pipeline_with_bias_and_experimentation_tracking_xlm_robert",
    description="Model data pipeline in TF",
    pipeline_root=_pipeline_artifacts_dir,
)
def model_data_pipeline(start_year: int = 2018, end_year: int = 2023):
    # Step 1: Get data
    get_data_component_task = get_data_component(
        project_id=PROJECT_ID,
        location=LOCATION,
        start_year=start_year,
        end_year=end_year,
        feature_name='complaints',
        label_name='product'
    )

    # Step 2: Prepare training and validation datasets
    train_data_prep_task = prepare_data_component(
        data=get_data_component_task.outputs['train_data'],
        dataset_name='train',
        feature_name='complaints',
        label_name='product',
        label_map=label_2_idx_map
    )
    validation_data_prep_task = prepare_data_component(
        data=get_data_component_task.outputs['val_data'],
        dataset_name='val',
        feature_name='complaints',
        label_name='product',
        label_map=label_2_idx_map
    )

    # Step 3: Train the mBERT model
    train_lstm_task = train_mbert_model(
        train_data=train_data_prep_task.outputs['tf_dataset'],
        model_params=bert_model_params,
        label_map=label_2_idx_map,
        train_data_name='train',
    )

    # Step 4: Test the model
    test_task = test_mbert_model(
        test_data=validation_data_prep_task.outputs['tf_dataset'],
        trained_model=train_lstm_task.outputs['model_output'],
        test_data_name='val',
        label_map=label_2_idx_map
    )
    experiment_tracking = log_metrics_to_vertex_ai(
      metrics_dict=test_task.outputs['metrics_dict']
    )


    # Step 5 : Bias detection
    bias_detection_task = bias_detection(
        test_data=validation_data_prep_task.outputs['tf_dataset'],
        trained_model=train_lstm_task.outputs['model_output'],
        feature_name='complaints',
        label_name='product',
        test_data_name='val',
        label_map=label_2_idx_map
    )
    bias_detection_task.after(test_task)

    # Step 6: Register the model in the Vertex AI Model Registry
    register_model_task = model_registration(
        model_output=test_task.outputs['reusable_model'],  # Use trained model output
        project_id=PROJECT_ID,
        location=LOCATION,
        model_display_name="tf_xlm_robert_model_product",
    )
    register_model_task.after(bias_detection_task)

    # Step 7: Deploy the registered model
    deploy_model_task = model_deployment(
        model=register_model_task.outputs['model'],  # Use registered model
        project_id=PROJECT_ID,
        location=LOCATION,
        endpoint_display_name="tf-xlmm_robert-endpoint-product",
        deployed_model_display_name="tf-xlm-robert-deployed-product",
    )

    # Log output for visibility
    print(f"Model Registration: {register_model_task.outputs['model']}")
    print(f"Model Deployment Endpoint: {deploy_model_task.outputs['endpoint']}")

compiler.Compiler().compile(
    pipeline_func=model_data_pipeline, package_path="model_deployment_pipeline_department_job.json"
)

TIMESTAMP = datetime.now().strftime("%Y%m%d%H%M%S")

job = aiplatform.PipelineJob(
    display_name="model_deployment_pipeline_department",
    template_path="model_deployment_pipeline_department_job.json",
    job_id="model-deployment-department-pipeline-{0}".format(TIMESTAMP),
    enable_caching=True
)

job.submit()